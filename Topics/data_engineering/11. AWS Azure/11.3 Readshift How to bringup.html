<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTFâ€‘8" />
  <title>AmazonÂ Redshift â€” Complete Handsâ€‘On Guide</title>
  <style>
    :root {
      --brand:#c10e3f;
      --accent:#005792;
      --bg:#f7f9fc;
      --code:#eef3ff;
    }
    body{font-family:system-ui,-apple-system,Segoe UI,Roboto,sans-serif;margin:32px;background:var(--bg);color:#333}
    h1{color:var(--brand);margin-top:0;font-size:2rem}
    h2{color:var(--accent);margin-top:2rem;border-bottom:2px solid #cbd5e1;padding-bottom:4px}
    h3{color:#444;margin-top:1.2rem}
    pre{background:var(--code);padding:12px;border-radius:6px;overflow-x:auto}
    code{background:#eef;padding:2px 4px;border-radius:4px}
    nav ul{padding-left:20px}
    nav li{margin:6px 0}
    a{color:var(--accent)}
    section{margin-bottom:36px}
    table{border-collapse:collapse;width:100%}
    th,td{border:1px solid #d9e2ec;padding:6px}
    th{background:#e0ebff}
  </style>
</head>
<body>

<h1>AmazonÂ RedshiftÂ ğŸ› Â â€”Â Create, Connect withÂ Python &amp; Operate LikeÂ aÂ Pro</h1>

<nav>
  <h2>TableÂ ofÂ Contents</h2>
  <ul>
    <li><a href="#create-console">1Â Â·Â Create a Cluster (AWSÂ Console)</a></li>
    <li><a href="#create-cli">2Â Â·Â Create via AWSÂ CLI / CloudFormation / Terraform</a></li>
    <li><a href="#python-access">3Â Â·Â Access Redshift withÂ Python</a></li>
    <li><a href="#operations">4Â Â·Â Daily Operations &amp; BestÂ Practices</a></li>
    <li><a href="#sql-cheatsheet">5Â Â·Â RedshiftÂ SQL Cheatâ€‘Sheet</a></li>
    <li><a href="#security">6Â Â·Â Security &amp; Governance</a></li>
    <li><a href="#cost-control">7Â Â·Â Cost Control &amp; Scaling</a></li>
    <li><a href="#troubleshoot">8Â Â·Â Troubleshooting &amp; Monitoring</a></li>
  </ul>
</nav>

<section id="create-console">
  <h2>1Â Â·Â Creating a Redshift Cluster via AWSÂ Console</h2>
  <ol>
    <li><strong>Login</strong> to the AWSÂ Console â†’ <em>Services â†’ Redshift</em> â†’ <em>CreateÂ cluster</em>.</li>
    <li><strong>ClusterÂ identifier</strong>: <code>analyticsâ€‘prod</code>.</li>
    <li><strong>Node type</strong>:
      <ul>
        <li><em>RA3</em> (recommended for separation of storage/compute, autoâ€‘scaling).</li>
        <li><em>dc2.large / dc2.8xlarge</em> for smaller, fixedâ€‘storage workloads.</li>
      </ul>
    </li>
    <li><strong>Number of nodes</strong>: start withÂ 1 for RA3 (itâ€™s still aÂ multiâ€‘AZ managed service under the hood).</li>
    <li><strong>Admin user &amp; password</strong>: e.g. <code>master</code>Â / <code>&lt;StrongPW&gt;</code>.</li>
    <li><strong>Network &amp; security</strong>:
      <ul>
        <li>Select an existing <strong>VPC</strong> or let Redshift create one.</li>
        <li>Create / choose a <strong>Subnet group</strong> covering at least two AZs.</li>
        <li>Add an inbound rule in the <strong>security group</strong>:
          <code>TCPÂ 5439</code> from your office / VPN CIDR.</li>
      </ul>
    </li>
    <li>(Optional) Tick <strong>Use default IAM Role</strong> (<code>AmazonRedshiftAllCommandsFullAccess</code>) so COPY/UNLOAD can read/write S3.</li>
    <li>Click <strong>Create cluster</strong> â†’ inÂ ~5â€‘10â€¯minutes the cluster is <em>Available</em>.</li>
    <li>Note the <strong>Endpoint</strong>: <code>analyticsâ€‘prod.abc123.usâ€‘eastâ€‘1.redshift.amazonaws.com:5439</code>.</li>
  </ol>
</section>

<section id="create-cli">
  <h2>2Â Â·Â Create Programmatically (CLI / CFN / Terraform)</h2>
  <h3>AWSÂ CLI</h3>
<pre><code># create-subnet-group once
aws redshift create-cluster-subnet-group \
  --cluster-subnet-group-name prod-sg \
  --description "Redshift subnets" \
  --subnet-ids subnet-aaa subnet-bbb

# launch cluster
aws redshift create-cluster \
  --cluster-identifier analytics-prod \
  --node-type ra3.xlplus \
  --number-of-nodes 2 \
  --master-username master \
  --master-user-password &lt;StrongPW&gt; \
  --cluster-subnet-group-name prod-sg \
  --iam-roles arn:aws:iam::123456789012:role/MyRedshiftRole
</code></pre>

  <h3>TerraformÂ (0.15+)</h3>
<pre><code>resource "aws_redshift_subnet_group" "prod" {
  name       = "prod-sg"
  subnet_ids = ["subnet-aaa","subnet-bbb"]
}

resource "aws_redshift_cluster" "prod" {
  cluster_identifier = "analytics-prod"
  node_type          = "ra3.xlplus"
  number_of_nodes    = 2
  master_username    = "master"
  master_password    = var.redshift_password
  port               = 5439
  iam_roles          = [aws_iam_role.redshift.arn]
  subnet_group_name  = aws_redshift_subnet_group.prod.name
}
</code></pre>
</section>

<section id="python-access">
  <h2>3Â Â·Â Accessing Redshift withÂ PythonÂ ğŸ</h2>
  <h3>3.1Â psycopg2Â /Â SQLAlchemy (JDBCâ€‘style)</h3>
<pre><code>pip install psycopg2-binary sqlalchemy pandas</code></pre>

<pre><code>import pandas as pd, sqlalchemy as sa

conn_str = (
    "redshift+psycopg2://master:&lt;PW&gt;"
    "@analytics-prod.abc123.us-east-1.redshift.amazonaws.com:5439/dev"
)

engine = sa.create_engine(conn_str)

df = pd.read_sql("SELECT first_name, total_spend FROM customers LIMIT 10", engine)
print(df.head())
</code></pre>

  <h3>3.2Â Redshift DataÂ API (no SQL drivers, works behindÂ NAT)</h3>
<pre><code>pip install boto3</code></pre>
<pre><code>import boto3, time, pandas as pd, io

rs = boto3.client("redshift-data")

def run(query, database="dev", cluster="analytics-prod"):
    resp = rs.execute_statement(Database=database, Sql=query,
                                ClusterIdentifier=cluster)
    qid = resp["Id"]
    # simple poll
    while (stat := rs.describe_statement(Id=qid))["Status"] in ["SUBMITTED","PICKED","STARTED"]:
        time.sleep(1)
    if stat["Status"] != "FINISHED":
        raise RuntimeError(stat["Error"])
    return rs.get_statement_result(Id=qid)

rows = run("SELECT count(*) FROM sales")["Records"]
print(rows[0][0]["longValue"])
</code></pre>
  <h3>3.3Â SpectrumÂ +Â GlueÂ catalog example</h3>
<pre><code>query = """
SELECT sales_date, sum(amount) AS total
FROM spectrum.sales_external
WHERE year = 2025
GROUP BY sales_date;
"""
pd.read_sql(query, engine)
</code></pre>
</section>

<section id="operations">
  <h2>4Â Â·Â Daily Operations &amp; BestÂ Practices</h2>
  <table>
    <thead><tr><th>Operation</th><th>HowÂ to</th><th>Notes</th></tr></thead>
    <tbody>
      <tr><td>Load data (S3Â â†’Â Redshift)</td><td><code>COPY s3://bucket/key ... IAM_ROLE ... FORMAT ASÂ CSV;</code></td><td>Split files intoÂ ~100â€¯MB partitions for max parallelism.</td></tr>
      <tr><td>Unload to S3</td><td><code>UNLOAD ('SELECTâ€¦') TO 's3://bucket/prefix' IAM_ROLE â€¦ PARQUET;</code></td><td>Use <code>PARQUET</code> for downstream Athena/Spark.</td></tr>
      <tr><td>Vacuum</td><td><code>VACUUM;</code></td><td>Run after massive deletes/inserts; RA3 uses autoâ€‘vacuum by default.</td></tr>
      <tr><td>Analyze</td><td><code>ANALYZE;</code></td><td>Updates table statistics for query planner.</td></tr>
      <tr><td>Resize cluster</td><td>AWSÂ Console â†’ <em>Modify</em> â†’ change nodeÂ count</td><td>Elastic Resize is nearâ€‘zeroâ€‘downtime; for RA3 you can add/remove quickly.</td></tr>
      <tr><td>Pause/Resume</td><td><code>aws redshift pause-cluster</code></td><td>Only for singleâ€‘node clusters; billing stops for compute.</td></tr>
      <tr><td>Snapshots</td><td>Automatic daily or <code>CREATE SNAPSHOT</code></td><td>Supports crossâ€‘region, crossâ€‘account.</td></tr>
      <tr><td>Data sharing</td><td>CONSOLE â†’Â <strong>Data share</strong></td><td>Share live data to other AWSÂ accounts without ETL.</td></tr>
      <tr><td>Audit</td><td>Enable <code>useractivitylog</code> to S3</td><td>GLUE â†’ Athena for querying logs.</td></tr>
    </tbody>
  </table>
</section>

<section id="sql-cheatsheet">
  <h2>5Â Â·Â Redshift SQL Cheatâ€‘Sheet</h2>
<pre><code>-- Distribution styles
CREATE TABLE facts (
  id BIGINT,
  amount DECIMAL(12,2)
)
DISTSTYLE KEY DISTKEY(id)
SORTKEY(id);

-- CTAS (faster than INSERTÂ SELECT)
CREATE TABLE big_sales
  DISTSTYLE ALL
AS
SELECT * FROM sales WHERE total &gt; 1000;

-- Spectrum external table
CREATE EXTERNAL TABLE spectrum.sales_external(
  sale_id bigint,
  sales_date date,
  amount decimal(10,2))
PARTITIONED BY (year int)
STORED AS PARQUET
LOCATION 's3://my-bucket/sales_parquet/';

-- COPY with JSONPaths
COPY staging
FROM 's3://bucket/raw/'
IAM_ROLE 'arn:aws:iam::123:role/redshift'
FORMAT AS JSON 'auto';

-- Concurrency scaling
ALTER SYSTEM SET concurrency_scaling_mode = auto;</code></pre>
</section>

<section id="security">
  <h2>6Â Â·Â SecurityÂ ğŸ”</h2>
  <ul>
    <li><strong>Encryption</strong>: Enable at cluster creation (KMS or HSM). All snapshots inherit.</li>
    <li><strong>VPC</strong>: Place cluster in private subnets, access through bastion/VPN or RedshiftÂ QueryÂ EditorÂ V2.</li>
    <li><strong>IAMÂ Role</strong>: Grant S3Â read/write (<code>AmazonS3ReadOnlyAccess</code>) minimally required.</li>
    <li><strong>Rowâ€‘level &amp; columnâ€‘level security</strong> (RLS/CLS) available since 2024 â€” define <code>CREATE ROW LEVEL SECURITY POLICY</code>.</li>
    <li><strong>Audit logging</strong>: Enable user / connection / useractivity logs to S3 + CloudTrail.</li>
  </ul>
</section>

<section id="cost-control">
  <h2>7Â Â·Â Cost Control &amp; Autoâ€‘Scaling</h2>
  <ul>
    <li><strong>RA3 Reserved Instances</strong>: upÂ toÂ 55â€¯% savings for 1â€‘ or 3â€‘year commitments.</li>
    <li><strong>Concurrency Scaling</strong>: FirstÂ 1â€¯hour/day free; good for unpredictable spikes.</li>
    <li><strong>Short query accelerationÂ (SQA)</strong>: Autoâ€‘isolates quick queries onto dedicated resources.</li>
    <li><strong>Pausable clusters</strong>: Singleâ€‘node clusters can pause compute billing when idle.</li>
    <li><strong>Use Spectrum</strong>: Keep infrequently accessed data inÂ S3 to avoid storage charges.</li>
  </ul>
</section>

<section id="troubleshoot">
  <h2>8Â Â·Â Troubleshooting &amp; Monitoring</h2>
  <h3>CloudWatch Metrics</h3>
  <ul>
    <li><code>CPUUtilization</code>Â &gt;Â 90â€¯% â†’ consider adding nodes.</li>
    <li><code>HealthStatus</code> &amp; <code>MaintenanceMode</code> events.</li>
  </ul>
  <h3>System Tables</h3>
<pre><code>-- Top 5 longest queries in last hour
SELECT pid, start_time, substring, elapsed
FROM stv_recents
ORDER BY elapsed DESC
LIMIT 5;

-- Disk usage per schema
SELECT schemaname, sum(capacity) / 1024 AS mb
FROM svv_table_info
GROUP BY schemaname;</code></pre>
  <h3>Workload ManagementÂ (WLM)</h3>
  <p>Edit parameter group â†’ define user queues, slot counts, memory %.</p>
</section>

<footer>
  <p style="text-align:center;color:#666;margin-top:40px">
    Â©Â 2025Â â€“Â Your Redshift Quickâ€‘Start Guide. Happy querying!
  </p>
</footer>

</body>
</html>
