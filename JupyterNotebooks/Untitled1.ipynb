{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc74f9f9-a2a4-40d9-ba5a-c59483fb6b14",
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (AccessDenied) when calling the GetClusterCredentials operation: User: arn:aws:sts::310879042055:assumed-role/AmazonRedshift-CommandsAccessRole-20250703T155944/RedshiftSession is not authorized to perform: redshift:GetClusterCredentials on resource: arn:aws:redshift:ap-south-1:310879042055:dbuser:demo-workgroup/admin because no identity-based policy allows the redshift:GetClusterCredentials action",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Generate temporary database credentials\u001b[39;00m\n\u001b[32m     29\u001b[39m db_user = \u001b[33m'\u001b[39m\u001b[33madmin\u001b[39m\u001b[33m'\u001b[39m  \u001b[38;5;66;03m# Replace with your Redshift database user (e.g., 'admin' or another user in 'dev')\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m response = \u001b[43mredshift_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_cluster_credentials\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mDbUser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdb_user\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mDbName\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdb_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mClusterIdentifier\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkgroup_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mAutoCreate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# JDBC connection string\u001b[39;00m\n\u001b[32m     38\u001b[39m jdbc_url = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mjdbc:redshift:iam://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworkgroup_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccount_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.redshift-serverless.amazonaws.com:5439/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages/botocore/client.py:595\u001b[39m, in \u001b[36mClientCreator._create_api_method.<locals>._api_call\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    591\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    592\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m() only accepts keyword arguments.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    593\u001b[39m     )\n\u001b[32m    594\u001b[39m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m595\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages/botocore/context.py:123\u001b[39m, in \u001b[36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[32m    122\u001b[39m     hook()\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages/botocore/client.py:1058\u001b[39m, in \u001b[36mBaseClient._make_api_call\u001b[39m\u001b[34m(self, operation_name, api_params)\u001b[39m\n\u001b[32m   1054\u001b[39m     error_code = error_info.get(\u001b[33m\"\u001b[39m\u001b[33mQueryErrorCode\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info.get(\n\u001b[32m   1055\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCode\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1056\u001b[39m     )\n\u001b[32m   1057\u001b[39m     error_class = \u001b[38;5;28mself\u001b[39m.exceptions.from_code(error_code)\n\u001b[32m-> \u001b[39m\u001b[32m1058\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[32m   1059\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1060\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[31mClientError\u001b[39m: An error occurred (AccessDenied) when calling the GetClusterCredentials operation: User: arn:aws:sts::310879042055:assumed-role/AmazonRedshift-CommandsAccessRole-20250703T155944/RedshiftSession is not authorized to perform: redshift:GetClusterCredentials on resource: arn:aws:redshift:ap-south-1:310879042055:dbuser:demo-workgroup/admin because no identity-based policy allows the redshift:GetClusterCredentials action"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import jaydebeapi\n",
    "\n",
    "# Redshift Serverless configuration\n",
    "region = 'ap-south-1'\n",
    "workgroup_name = 'demo-workgroup'\n",
    "namespace = 'demo-namespace'\n",
    "db_name = 'dev'\n",
    "account_id = '20250703T155944'\n",
    "iam_role = 'arn:aws:iam::310879042055:role/service-role/AmazonRedshift-CommandsAccessRole-20250703T155944'\n",
    "#'arn:aws:redshift-serverless:us-east-1:310879042055:namespace/281acd8d-bab1-40e0-b841-b44f5cf4b4ce'\n",
    "\n",
    "# Initialize boto3 client with the IAM role\n",
    "session = boto3.Session(region_name=region)\n",
    "sts_client = session.client('sts')\n",
    "assumed_role = sts_client.assume_role(RoleArn=iam_role, RoleSessionName='RedshiftSession')\n",
    "credentials = assumed_role['Credentials']\n",
    "\n",
    "# Configure boto3 client with temporary credentials\n",
    "redshift_client = boto3.client(\n",
    "    'redshift',\n",
    "    region_name=region,\n",
    "    aws_access_key_id=credentials['AccessKeyId'],\n",
    "    aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "    aws_session_token=credentials['SessionToken']\n",
    ")\n",
    "\n",
    "# Generate temporary database credentials\n",
    "db_user = 'admin'  # Replace with your Redshift database user (e.g., 'admin' or another user in 'dev')\n",
    "response = redshift_client.get_cluster_credentials(\n",
    "    DbUser=db_user,\n",
    "    DbName=db_name,\n",
    "    ClusterIdentifier=workgroup_name,\n",
    "    AutoCreate=False\n",
    ")\n",
    "\n",
    "# JDBC connection string\n",
    "jdbc_url = f'jdbc:redshift:iam://{workgroup_name}.{account_id}.{region}.redshift-serverless.amazonaws.com:5439/{db_name}'\n",
    "\n",
    "# Connect to Redshift using JDBC driver\n",
    "conn = jaydebeapi.connect(\n",
    "    'com.amazon.redshift.jdbc.Driver',\n",
    "    jdbc_url,\n",
    "    [response['DbUser'], response['DbPassword']],\n",
    "    'path/to/redshift-jdbc42-2.1.0.29.jar'  # Replace with path to JDBC driver\n",
    ")\n",
    "\n",
    "# Test the connection with a sample query\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('SELECT CURRENT_USER')\n",
    "print(f\"Connected as: {cursor.fetchone()[0]}\")\n",
    "\n",
    "# Clean up\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00f63e2-6485-445f-8cff-01fa8757408f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd03e5a-7fad-448b-9d1f-65656ff66670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32618b46-5ba0-4726-b536-a9a1ba2b869d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0dc257-66a7-4e64-9db1-dba549b4480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "{\n",
    "\t\"Version\": \"2012-10-17\",\n",
    "\t\"Statement\": [\n",
    "\t\t{\n",
    "\t\t\t\"Effect\": \"Allow\",\n",
    "\t\t\t\"Principal\": {\n",
    "\t\t\t\t\"Service\": [\n",
    "\t\t\t\t\t\"redshift-serverless.amazonaws.com\",\n",
    "\t\t\t\t\t\"sagemaker.amazonaws.com\",\n",
    "\t\t\t\t\t\"redshift.amazonaws.com\"\n",
    "\t\t\t\t]\n",
    "\t\t\t},\n",
    "\t\t\t\"Action\": \"sts:AssumeRole\"\n",
    "\t\t}\n",
    "\t]\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54d12ca-2cb9-487e-9f2d-5d3b78a72866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import time\n",
    "from io import StringIO\n",
    "import uuid\n",
    "\n",
    "# --- Redshift Serverless and S3 Configuration (replace with your actual details) ---\n",
    "AWS_REGION = 'ap-south-1' # Example: 'us-east-1'. Make sure this matches your Redshift region.\n",
    "REDSHIFT_SERVERLESS_WORKGROUP_NAME = 'demo-workgroup' # e.g., 'redshift-serverless-wg-prod'\n",
    "REDSHIFT_SERVERLESS_DB_NAME = 'dev' # The database name within your workgroup (e.g., 'dev' or 'master')\n",
    "# This is the database user that will execute the SQL. This user should exist in your Redshift DB.\n",
    "# It can be mapped to an IAM user/role for granular permissions within Redshift.\n",
    "REDSHIFT_DB_USER = 'IAM:RootIdentity' # e.g., 'admin_user' or 'analytics_role'\n",
    "\n",
    "S3_BUCKET_NAME = 'your-unique-redshift-serverless-upload-bucket' # Must be globally unique\n",
    "S3_KEY_PREFIX = 'redshift-data-uploads/' # Path within your S3 bucket\n",
    "\n",
    "# IAM Role ARN that your Redshift Serverless Namespace uses to access S3 for COPY/UNLOAD commands.\n",
    "# This role needs s3:GetObject permission on S3_BUCKET_NAME.\n",
    "IAM_ROLE_ARN_FOR_COPY = 'arn:aws:iam::123456789012:role/YourRedshiftServerlessCopyRole'\n",
    "\n",
    "\n",
    "def execute_redshift_data_api_statement(sql_statement, workgroup_name, database, db_user, wait_for_completion=True):\n",
    "    \"\"\"\n",
    "    Executes a SQL statement on Redshift Serverless using the Redshift Data API.\n",
    "    Optionally waits for the statement to complete and handles basic error reporting.\n",
    "    \"\"\"\n",
    "    client = boto3.client('redshift-data', region_name=AWS_REGION)\n",
    "\n",
    "    try:\n",
    "        response = client.execute_statement(\n",
    "            WorkgroupName=workgroup_name, # Specify the workgroup for Serverless\n",
    "            Database=database,\n",
    "            DbUser=db_user,\n",
    "            Sql=sql_statement,\n",
    "            WithEvent=False # Set to True if you want to use EventBridge for async notifications\n",
    "        )\n",
    "        statement_id = response['Id']\n",
    "        print(f\"Submitted SQL statement with ID: {statement_id}\")\n",
    "\n",
    "        if wait_for_completion:\n",
    "            status = ''\n",
    "            start_time = time.time()\n",
    "            # Poll for statement status (consider more sophisticated retry/backoff for production)\n",
    "            while status not in ['FINISHED', 'FAILED', 'ABORTED']:\n",
    "                time.sleep(2) # Wait 2 seconds before polling again\n",
    "                desc_response = client.describe_statement(Id=statement_id)\n",
    "                status = desc_response['Status']\n",
    "                print(f\"Statement {statement_id} status: {status} ({time.time() - start_time:.1f}s)\")\n",
    "\n",
    "                if status == 'FAILED':\n",
    "                    print(f\"Error executing statement: {desc_response.get('Error', 'Unknown error')}\")\n",
    "                    return None\n",
    "                elif status == 'ABORTED':\n",
    "                    print(f\"Statement was aborted.\")\n",
    "                    return None\n",
    "            print(f\"Statement {statement_id} finished in {time.time() - start_time:.1f} seconds.\")\n",
    "            return statement_id\n",
    "        else:\n",
    "            return statement_id # Return ID immediately if not waiting\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to execute statement via Redshift Data API: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_redshift_data_api_statement_results(statement_id):\n",
    "    \"\"\"Retrieves results of a completed Redshift Data API statement.\"\"\"\n",
    "    client = boto3.client('redshift-data', region_name=AWS_REGION)\n",
    "    try:\n",
    "        response = client.get_statement_result(Id=statement_id)\n",
    "        # 'Records' is a list of lists, where each inner list contains column values\n",
    "        # formatted as {'stringValue': '...', 'longValue': '...', etc.}\n",
    "        records = response.get('Records', [])\n",
    "\n",
    "        # For simple queries like listing databases, we can extract string values.\n",
    "        # For complex queries, you might need to iterate through column metadata and parse.\n",
    "        results = []\n",
    "        for record in records:\n",
    "            # Assuming the first value is the one we want for simple listings\n",
    "            if record and isinstance(record, list) and len(record) > 0:\n",
    "                if 'stringValue' in record[0]:\n",
    "                    results.append(record[0]['stringValue'])\n",
    "                elif 'longValue' in record[0]:\n",
    "                    results.append(record[0]['longValue'])\n",
    "                # Add other types as needed\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting statement results: {e}\")\n",
    "        return []\n",
    "\n",
    "def list_redshift_serverless_databases(workgroup_name, database, db_user):\n",
    "    \"\"\"Lists databases in Redshift Serverless using the Data API.\"\"\"\n",
    "    sql = \"SELECT datname FROM pg_database;\"\n",
    "    statement_id = execute_redshift_data_api_statement(sql, workgroup_name, database, db_user)\n",
    "\n",
    "    if statement_id:\n",
    "        databases = get_redshift_data_api_statement_results(statement_id)\n",
    "        print(\"\\nRedshift Serverless Databases:\")\n",
    "        if databases:\n",
    "            for db in databases:\n",
    "                print(f\"- {db}\")\n",
    "        else:\n",
    "            print(\"No databases found or results could not be retrieved.\")\n",
    "        return databases\n",
    "    return []\n",
    "\n",
    "def upload_dataframe_to_redshift_serverless(\n",
    "    df,\n",
    "    table_name,\n",
    "    workgroup_name,\n",
    "    database,\n",
    "    db_user,\n",
    "    s3_bucket,\n",
    "    s3_key_prefix,\n",
    "    iam_role_arn_for_copy,\n",
    "    create_table_sql=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Uploads a pandas DataFrame to Redshift Serverless using S3 and the COPY command via Data API.\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3', region_name=AWS_REGION)\n",
    "\n",
    "    # Convert DataFrame to CSV in memory (without header or index for COPY)\n",
    "    csv_buffer = StringIO()\n",
    "    df.to_csv(csv_buffer, index=False, header=False, sep=',')\n",
    "\n",
    "    # Generate a unique S3 object key\n",
    "    s3_file_name = f\"{s3_key_prefix}{table_name}_{uuid.uuid4()}.csv\"\n",
    "    s3_path = f\"s3://{s3_bucket}/{s3_file_name}\"\n",
    "\n",
    "    try:\n",
    "        # 1. Upload DataFrame data to S3\n",
    "        print(f\"Uploading data to S3: {s3_path}...\")\n",
    "        s3_client.put_object(Bucket=s3_bucket, Key=s3_file_name, Body=csv_buffer.getvalue())\n",
    "        print(f\"Data successfully uploaded to S3.\")\n",
    "\n",
    "        # 2. (Optional) Create table if it doesn't exist\n",
    "        if create_table_sql:\n",
    "            print(f\"Checking/Creating table '{table_name}'...\")\n",
    "            create_stmt_id = execute_redshift_data_api_statement(\n",
    "                create_table_sql, workgroup_name, database, db_user\n",
    "            )\n",
    "            if create_stmt_id:\n",
    "                print(f\"Table creation statement processed for '{table_name}'.\")\n",
    "            else:\n",
    "                print(f\"Failed to process table creation statement for '{table_name}'. Data load might fail.\")\n",
    "\n",
    "        # 3. Execute Redshift COPY command via Data API\n",
    "        copy_command = f\"\"\"\n",
    "        COPY {table_name}\n",
    "        FROM '{s3_path}'\n",
    "        IAM_ROLE '{iam_role_arn_for_copy}'\n",
    "        CSV\n",
    "        IGNOREHEADER 0;\n",
    "        \"\"\"\n",
    "        print(f\"Executing COPY command for table '{table_name}'...\")\n",
    "        copy_stmt_id = execute_redshift_data_api_statement(\n",
    "            copy_command, workgroup_name, database, db_user\n",
    "        )\n",
    "\n",
    "        if copy_stmt_id:\n",
    "            print(f\"Data successfully loaded into Redshift Serverless table '{table_name}'.\")\n",
    "            # 4. (Optional) Clean up temporary S3 file\n",
    "            s3_client.delete_object(Bucket=s3_bucket, Key=s3_file_name)\n",
    "            print(f\"Temporary S3 file deleted: {s3_path}\")\n",
    "        else:\n",
    "            print(f\"Data load into '{table_name}' failed or was aborted.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during data upload: {e}\")\n",
    "    finally:\n",
    "        csv_buffer.close()\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure your AWS credentials are configured (e.g., via `aws configure` in your terminal,\n",
    "    # or by setting environment variables like AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY,\n",
    "    # or if running on an EC2 instance/Lambda, attach an IAM role with necessary permissions).\n",
    "\n",
    "    print(\"Attempting to list databases in Redshift Serverless...\")\n",
    "    listed_databases = list_redshift_serverless_databases(\n",
    "        REDSHIFT_SERVERLESS_WORKGROUP_NAME,\n",
    "        REDSHIFT_SERVERLESS_DB_NAME,\n",
    "        REDSHIFT_DB_USER\n",
    "    )\n",
    "\n",
    "    # Example DataFrame to upload\n",
    "    sample_data = {\n",
    "        'event_id': [1, 2, 3, 4],\n",
    "        'event_name': ['Login', 'Logout', 'Purchase', 'PageView'],\n",
    "        'timestamp': ['2025-07-03 10:00:00', '2025-07-03 10:30:00', '2025-07-03 11:00:00', '2025-07-03 11:15:00'],\n",
    "        'user_id': [101, 101, 102, 103]\n",
    "    }\n",
    "    df_to_upload = pd.DataFrame(sample_data)\n",
    "    target_table = 'user_events'\n",
    "\n",
    "    # SQL to create the target table if it doesn't exist\n",
    "    # Adjust column names and data types as per your DataFrame\n",
    "    create_table_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {target_table} (\n",
    "        event_id INT,\n",
    "        event_name VARCHAR(255),\n",
    "        timestamp TIMESTAMP,\n",
    "        user_id INT\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\nAttempting to upload data to table: {target_table}\")\n",
    "    upload_dataframe_to_redshift_serverless(\n",
    "        df_to_upload,\n",
    "        target_table,\n",
    "        REDSHIFT_SERVERLESS_WORKGROUP_NAME,\n",
    "        REDSHIFT_SERVERLESS_DB_NAME,\n",
    "        REDSHIFT_DB_USER,\n",
    "        S3_BUCKET_NAME,\n",
    "        S3_KEY_PREFIX,\n",
    "        IAM_ROLE_ARN_FOR_COPY,\n",
    "        create_table_sql=create_table_sql\n",
    "    )\n",
    "\n",
    "    print(\"\\nScript execution complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
