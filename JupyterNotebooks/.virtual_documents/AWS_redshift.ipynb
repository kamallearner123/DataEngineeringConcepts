import redshift_connector
import boto3
import os
import pandas as pd # Optional, for showing dataframes

# --- Configuration ---
# Replace with your actual Redshift cluster details
CLUSTER_IDENTIFIER = 'your-redshift-cluster-id'
DATABASE_NAME = 'kamal' # Or your specific database name
DB_USER = 'postgres' # A Redshift database user that exists in your cluster
AWS_REGION = 'ap-south-1' # e.g., 'us-east-1'

# S3 bucket for COPY/UNLOAD (must exist and have Redshift role access)
S3_BUCKET_NAME = 'kkm2-unique-test-bucket-2025-06-26-py-1'
S3_FILE_TO_UNLOAD = 'redshift_sample.csv'
S3_UNLOAD_PATH = f's3://{S3_BUCKET_NAME}/{S3_FILE_TO_UNLOAD}'
S3_COPY_PATH = f's3://{S3_BUCKET_NAME}/flights.csv' # Path to your existing sample data

# Ensure your AWS CLI is configured or environment variables are set
# AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_DEFAULT_REGION

print("--- Starting Redshift Python Demo ---")

# --- 1. Securely Connect using IAM Authentication ---
# This is the recommended way. redshift_connector uses boto3's underlying
# credential chain (environment variables, ~/.aws/credentials, IAM roles).
print("\n1. Establishing secure connection to Redshift using IAM...")
try:
    conn = redshift_connector.connect(
        iam=True, # Enable IAM authentication
        cluster_identifier=CLUSTER_IDENTIFIER,
        database=DATABASE_NAME,
        db_user=DB_USER, # The Redshift database user to authenticate as
        region=AWS_REGION
    )
    print("Connection successful!")
    cursor = conn.cursor()

except Exception as e:
    print(f"Error connecting to Redshift: {e}")
    print("Please ensure your AWS CLI is configured, Redshift cluster is accessible,")
    print("and the IAM user/role has 'redshift:GetClusterCredentials' permission.")
    exit() # Exit if connection fails, as nothing else will work

# --- 2. Execute a Simple Query ---
print("\n2. Executing a simple SELECT query:")
try:
    cursor.execute("SELECT id, name, city FROM users LIMIT 5;")
    results = cursor.fetchall()

    if results:
        print("Query Results:")
        for row in results:
            print(row)
    else:
        print("No results found.")
except Exception as e:
    print(f"Error executing query: {e}")

# --- 3. Parameterized Query (Prevent SQL Injection) ---
print("\n3. Demonstrating a parameterized query:")
try:
    city_filter = 'London'
    cursor.execute("SELECT id, name FROM users WHERE city = %s;", (city_filter,))
    filtered_results = cursor.fetchall()
    print(f"Users in {city_filter}:")
    for row in filtered_results:
        print(row)
except Exception as e:
    print(f"Error with parameterized query: {e}")

# --- 4. INSERT Data ---
print("\n4. Inserting new data:")
try:
    new_user_id = 4
    new_user_name = 'Alice Wonderland'
    new_user_city = 'Seattle'
    cursor.execute("INSERT INTO users (id, name, city) VALUES (%s, %s, %s);",
                   (new_user_id, new_user_name, new_user_city))
    conn.commit() # Commit the transaction
    print(f"Inserted user: {new_user_name}")

    # Verify insertion
    cursor.execute(f"SELECT * FROM users WHERE id = {new_user_id};")
    print("Verifying insertion:")
    print(cursor.fetchone())

except Exception as e:
    conn.rollback() # Rollback on error
    print(f"Error inserting data: {e}")

# --- 5. UNLOAD Data to S3 ---
# Requires IAM role associated with Redshift cluster to have S3 write permissions
print("\n5. Unloading data to S3:")
try:
    # Ensure the IAM role attached to your Redshift cluster has write access to S3_BUCKET_NAME
    # Example: 'arn:aws:iam::ACCOUNT_ID:role/YourRedshiftClusterIAMRole'
    iam_role_arn = 'arn:aws:iam::YOUR_AWS_ACCOUNT_ID:role/YourRedshiftClusterIAMRole' # REPLACE THIS

    unload_query = f"""
    UNLOAD ('SELECT id, name, city FROM users')
    TO '{S3_UNLOAD_PATH}'
    IAM_ROLE '{iam_role_arn}'
    CSV
    HEADER
    PARALLEL OFF; -- For small demos, PARALLEL OFF makes it one file. Remove for large datasets.
    """
    print(f"Executing UNLOAD query to {S3_UNLOAD_PATH}...")
    cursor.execute(unload_query)
    conn.commit()
    print("Data successfully unloaded to S3. Check your S3 bucket!")
    # Optional: Verify S3 file exists using boto3
    s3_client = boto3.client('s3', region_name=AWS_REGION)
    try:
        s3_client.head_object(Bucket=S3_BUCKET_NAME, Key=S3_FILE_TO_UNLOAD + '000') # Check for part file
        print(f"Confirmed file {S3_FILE_TO_UNLOAD} exists in S3.")
    except Exception as s3_e:
        print(f"Could not confirm S3 file presence (may take a moment to appear): {s3_e}")

except Exception as e:
    conn.rollback()
    print(f"Error unloading data to S3: {e}")
    print("Ensure the IAM role associated with your Redshift cluster has S3 write permissions.")

# --- 6. (Optional) Show Data API usage with boto3 ---
# This is an alternative to persistent connections, good for serverless/asynchronous needs.
print("\n6. (Optional) Demonstrating Redshift Data API (using boto3):")
try:
    redshift_data_client = boto3.client('redshift-data', region_name=AWS_REGION)

    # Execute a statement
    statement_response = redshift_data_client.execute_statement(
        ClusterIdentifier=CLUSTER_IDENTIFIER,
        Database=DATABASE_NAME,
        DbUser=DB_USER, # The Redshift database user
        Sql="SELECT * FROM users WHERE id = 1;"
    )
    statement_id = statement_response['Id']
    print(f"Data API Statement ID: {statement_id}")

    # Wait for statement to complete (for demo purposes)
    import time
    status = ''
    while status not in ['FINISHED', 'FAILED', 'ABORTED']:
        time.sleep(1)
        description_response = redshift_data_client.describe_statement(Id=statement_id)
        status = description_response['Status']
        print(f"Statement status: {status}")

    if status == 'FINISHED':
        result_response = redshift_data_client.get_statement_result(Id=statement_id)
        print("Data API Results:")
        # Data API results are in a specific format, you'd usually parse them
        if result_response and result_response.get('Records'):
            # Extract column names
            column_names = [col['label'] for col in result_response['ColumnMetadata']]
            print(column_names)
            for record in result_response['Records']:
                # Each record is a list of dictionaries, where each dict has a key matching the data type (e.g., 'longValue', 'stringValue')
                row_values = [list(col.values())[0] for col in record]
                print(row_values)
        else:
            print("No results from Data API.")
    else:
        print(f"Data API query failed or was aborted: {description_response.get('Error')}")

except Exception as e:
    print(f"Error using Redshift Data API: {e}")
    print("Ensure the IAM user/role has 'redshift-data:*' permissions.")


# --- Cleanup ---
print("\n--- Cleaning up ---")
try:
    cursor.close()
    conn.close()
    print("Connection closed.")
except Exception as e:
    print(f"Error during cleanup: {e}")

print("\n--- Demo Complete! ---")



