{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a2bf95d-a5c9-4c0e-bac5-6fbdeaca4ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Redshift Python Demo ---\n",
      "\n",
      "1. Establishing secure connection to Redshift using IAM...\n",
      "Error connecting to Redshift: connect() got an unexpected keyword argument 'workgroup_name'\n",
      "Please ensure your AWS CLI is configured, Redshift cluster is accessible,\n",
      "and the IAM user/role has 'redshift:GetClusterCredentials' permission.\n",
      "\n",
      "2. Executing a simple SELECT query:\n",
      "Error executing query: name 'cursor' is not defined\n",
      "\n",
      "3. Demonstrating a parameterized query:\n",
      "Error with parameterized query: name 'cursor' is not defined\n",
      "\n",
      "4. Inserting new data:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'conn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 99\u001b[39m\n\u001b[32m     98\u001b[39m new_user_city = \u001b[33m'\u001b[39m\u001b[33mSeattle\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m \u001b[43mcursor\u001b[49m.execute(\u001b[33m\"\u001b[39m\u001b[33mINSERT INTO users (id, name, city) VALUES (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m);\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    100\u001b[39m                (new_user_id, new_user_name, new_user_city))\n\u001b[32m    101\u001b[39m conn.commit() \u001b[38;5;66;03m# Commit the transaction\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'cursor' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 110\u001b[39m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28mprint\u001b[39m(cursor.fetchone())\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     \u001b[43mconn\u001b[49m.rollback() \u001b[38;5;66;03m# Rollback on error\u001b[39;00m\n\u001b[32m    111\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError inserting data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# --- 5. UNLOAD Data to S3 ---\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# Requires IAM role associated with Redshift cluster to have S3 write permissions\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'conn' is not defined"
     ]
    }
   ],
   "source": [
    "import redshift_connector\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd # Optional, for showing dataframes\n",
    "\n",
    "# --- Configuration ---\n",
    "# Replace with your actual Redshift cluster details\n",
    "DATABASE_NAME='dev' # Or your specific database name\n",
    "DB_USER='admin' # A Redshift database user that exists in your cluster\n",
    "AWS_REGION='us-east-1' # e.g., 'us-east-1'\n",
    "REDSHIFT_SERVERLESS_WORKGROUP_NAME='demo-workgroup-01'\n",
    "\n",
    "# S3 bucket for COPY/UNLOAD (must exist and have Redshift role access)\n",
    "S3_BUCKET_NAME = 'kkm2-unique-test-bucket-2025-06-26-py-1'\n",
    "S3_FILE_TO_UNLOAD = 'redshift_sample.csv'\n",
    "S3_UNLOAD_PATH = f's3://{S3_BUCKET_NAME}/{S3_FILE_TO_UNLOAD}'\n",
    "S3_COPY_PATH = f's3://{S3_BUCKET_NAME}/flights.csv' # Path to your existing sample data\n",
    "\n",
    "\n",
    "\n",
    "# Endpoint\n",
    "# default-workgroup.310879042055.us-east-1.redshift-serverless.amazonaws.com:5439/dev\n",
    "# JDBC URL\n",
    "# jdbc:redshift://default-workgroup.310879042055.us-east-1.redshift-serverless.amazonaws.com:5439/dev\n",
    "# ODBC URL\n",
    "# Driver={Amazon Redshift (x64)}; Server=default-workgroup.310879042055.us-east-1.redshift-serverless.amazonaws.com; Database=dev\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ensure your AWS CLI is configured or environment variables are set\n",
    "# AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_DEFAULT_REGION\n",
    "\n",
    "print(\"--- Starting Redshift Python Demo ---\")\n",
    "\n",
    "# --- 1. Securely Connect using IAM Authentication ---\n",
    "# This is the recommended way. redshift_connector uses boto3's underlying\n",
    "# credential chain (environment variables, ~/.aws/credentials, IAM roles).\n",
    "print(\"\\n1. Establishing secure connection to Redshift using IAM...\")\n",
    "try:\n",
    "    conn = redshift_connector.connect(\n",
    "        iam=True,\n",
    "        # For SERVERLESS, use workgroup_name\n",
    "        workgroup_name=REDSHIFT_SERVERLESS_WORKGROUP_NAME, # <<<<<< Make sure this line is present\n",
    "        database=DATABASE_NAME,\n",
    "        db_user=DB_USER,\n",
    "        region=AWS_REGION\n",
    "    )\n",
    "\n",
    "    # conn = redshift_connector.connect(\n",
    "    #     iam=True, # Enable IAM authentication\n",
    "    #     # workgroup_name=WORKGROUP_NAME, # Specify the Redshift Serverless Workgroup\n",
    "    #     database=DATABASE_NAME,\n",
    "    #     # db_user=DB_USER, # Often not needed if using IAM authentication\n",
    "    #     region=AWS_REGION\n",
    "    # )\n",
    "    print(\"REDSHIFT: Connection successful!\")\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Redshift: {e}\")\n",
    "    print(\"Please ensure your AWS CLI is configured, Redshift cluster is accessible,\")\n",
    "    print(\"and the IAM user/role has 'redshift:GetClusterCredentials' permission.\")\n",
    "    exit() # Exit if connection fails, as nothing else will work\n",
    "\n",
    "# --- 2. Execute a Simple Query ---\n",
    "print(\"\\n2. Executing a simple SELECT query:\")\n",
    "try:\n",
    "    cursor.execute(\"SELECT id, name, city FROM users LIMIT 5;\")\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    if results:\n",
    "        print(\"Query Results:\")\n",
    "        for row in results:\n",
    "            print(row)\n",
    "    else:\n",
    "        print(\"No results found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error executing query: {e}\")\n",
    "\n",
    "# --- 3. Parameterized Query (Prevent SQL Injection) ---\n",
    "print(\"\\n3. Demonstrating a parameterized query:\")\n",
    "try:\n",
    "    city_filter = 'London'\n",
    "    cursor.execute(\"SELECT id, name FROM users WHERE city = %s;\", (city_filter,))\n",
    "    filtered_results = cursor.fetchall()\n",
    "    print(f\"Users in {city_filter}:\")\n",
    "    for row in filtered_results:\n",
    "        print(row)\n",
    "except Exception as e:\n",
    "    print(f\"Error with parameterized query: {e}\")\n",
    "\n",
    "# --- 4. INSERT Data ---\n",
    "print(\"\\n4. Inserting new data:\")\n",
    "try:\n",
    "    new_user_id = 4\n",
    "    new_user_name = 'Alice Wonderland'\n",
    "    new_user_city = 'Seattle'\n",
    "    cursor.execute(\"INSERT INTO users (id, name, city) VALUES (%s, %s, %s);\",\n",
    "                   (new_user_id, new_user_name, new_user_city))\n",
    "    conn.commit() # Commit the transaction\n",
    "    print(f\"Inserted user: {new_user_name}\")\n",
    "\n",
    "    # Verify insertion\n",
    "    cursor.execute(f\"SELECT * FROM users WHERE id = {new_user_id};\")\n",
    "    print(\"Verifying insertion:\")\n",
    "    print(cursor.fetchone())\n",
    "\n",
    "except Exception as e:\n",
    "    conn.rollback() # Rollback on error\n",
    "    print(f\"Error inserting data: {e}\")\n",
    "\n",
    "# --- 5. UNLOAD Data to S3 ---\n",
    "# Requires IAM role associated with Redshift cluster to have S3 write permissions\n",
    "print(\"\\n5. Unloading data to S3:\")\n",
    "try:\n",
    "    # Ensure the IAM role attached to your Redshift cluster has write access to S3_BUCKET_NAME\n",
    "    # Example: 'arn:aws:iam::ACCOUNT_ID:role/YourRedshiftClusterIAMRole'\n",
    "    iam_role_arn = 'arn:aws:iam::YOUR_AWS_ACCOUNT_ID:role/YourRedshiftClusterIAMRole' # REPLACE THIS\n",
    "\n",
    "    unload_query = f\"\"\"\n",
    "    UNLOAD ('SELECT id, name, city FROM users')\n",
    "    TO '{S3_UNLOAD_PATH}'\n",
    "    IAM_ROLE '{iam_role_arn}'\n",
    "    CSV\n",
    "    HEADER\n",
    "    PARALLEL OFF; -- For small demos, PARALLEL OFF makes it one file. Remove for large datasets.\n",
    "    \"\"\"\n",
    "    print(f\"Executing UNLOAD query to {S3_UNLOAD_PATH}...\")\n",
    "    cursor.execute(unload_query)\n",
    "    conn.commit()\n",
    "    print(\"Data successfully unloaded to S3. Check your S3 bucket!\")\n",
    "    # Optional: Verify S3 file exists using boto3\n",
    "    s3_client = boto3.client('s3', region_name=AWS_REGION)\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=S3_BUCKET_NAME, Key=S3_FILE_TO_UNLOAD + '000') # Check for part file\n",
    "        print(f\"Confirmed file {S3_FILE_TO_UNLOAD} exists in S3.\")\n",
    "    except Exception as s3_e:\n",
    "        print(f\"Could not confirm S3 file presence (may take a moment to appear): {s3_e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    conn.rollback()\n",
    "    print(f\"Error unloading data to S3: {e}\")\n",
    "    print(\"Ensure the IAM role associated with your Redshift cluster has S3 write permissions.\")\n",
    "\n",
    "# --- 6. (Optional) Show Data API usage with boto3 ---\n",
    "# This is an alternative to persistent connections, good for serverless/asynchronous needs.\n",
    "print(\"\\n6. (Optional) Demonstrating Redshift Data API (using boto3):\")\n",
    "try:\n",
    "    redshift_data_client = boto3.client('redshift-data', region_name=AWS_REGION)\n",
    "\n",
    "    # Execute a statement\n",
    "    statement_response = redshift_data_client.execute_statement(\n",
    "        ClusterIdentifier=CLUSTER_IDENTIFIER,\n",
    "        Database=DATABASE_NAME,\n",
    "        DbUser=DB_USER, # The Redshift database user\n",
    "        Sql=\"SELECT * FROM users WHERE id = 1;\"\n",
    "    )\n",
    "    statement_id = statement_response['Id']\n",
    "    print(f\"Data API Statement ID: {statement_id}\")\n",
    "\n",
    "    # Wait for statement to complete (for demo purposes)\n",
    "    import time\n",
    "    status = ''\n",
    "    while status not in ['FINISHED', 'FAILED', 'ABORTED']:\n",
    "        time.sleep(1)\n",
    "        description_response = redshift_data_client.describe_statement(Id=statement_id)\n",
    "        status = description_response['Status']\n",
    "        print(f\"Statement status: {status}\")\n",
    "\n",
    "    if status == 'FINISHED':\n",
    "        result_response = redshift_data_client.get_statement_result(Id=statement_id)\n",
    "        print(\"Data API Results:\")\n",
    "        # Data API results are in a specific format, you'd usually parse them\n",
    "        if result_response and result_response.get('Records'):\n",
    "            # Extract column names\n",
    "            column_names = [col['label'] for col in result_response['ColumnMetadata']]\n",
    "            print(column_names)\n",
    "            for record in result_response['Records']:\n",
    "                # Each record is a list of dictionaries, where each dict has a key matching the data type (e.g., 'longValue', 'stringValue')\n",
    "                row_values = [list(col.values())[0] for col in record]\n",
    "                print(row_values)\n",
    "        else:\n",
    "            print(\"No results from Data API.\")\n",
    "    else:\n",
    "        print(f\"Data API query failed or was aborted: {description_response.get('Error')}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error using Redshift Data API: {e}\")\n",
    "    print(\"Ensure the IAM user/role has 'redshift-data:*' permissions.\")\n",
    "\n",
    "\n",
    "# --- Cleanup ---\n",
    "print(\"\\n--- Cleaning up ---\")\n",
    "try:\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(\"Connection closed.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during cleanup: {e}\")\n",
    "\n",
    "print(\"\\n--- Demo Complete! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31a418fb-f18c-4e00-be27-b1e8876a6dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Redshift Serverless!\n",
      "Current date in Redshift: 2025-07-08\n",
      "Current user and database name>  ('admin', 'dev')\n",
      "Schemaas  None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Redshift Serverless connection details\n",
    "host = \"default-workgroup.310879042055.us-east-1.redshift-serverless.amazonaws.com\"\n",
    "#:5439/dev\"\n",
    "#'<your-workgroup-name>.<random>.redshift-serverless.<region>.amazonaws.com'\n",
    "\n",
    "port = 5439\n",
    "dbname = 'dev'\n",
    "user = 'admin'\n",
    "password = 'KkmAcl4526'\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        host=host,\n",
    "        port=port,\n",
    "        dbname=dbname,\n",
    "        user=user,\n",
    "        password=password\n",
    "    )\n",
    "    print(\"Connected to Redshift Serverless!\")\n",
    "\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT current_date;\")\n",
    "    result = cur.fetchone()\n",
    "    print(\"Current date in Redshift:\", result[0])\n",
    "\n",
    "    cur.execute(\"SELECT current_user, current_database();\")\n",
    "    result = cur.fetchone()\n",
    "    print(\"Current user and database name> \", result)\n",
    "\n",
    "    cur.execute(\"SELECT schema_name FROM information_schema.schemata ORDER BY 1;\")\n",
    "    result = cur.fetchone()\n",
    "    print(\"Schemaas \", result)\n",
    "\n",
    "    cur.execute('''SELECT\n",
    "    trim(n.nspname)   AS schema,\n",
    "    trim(t.relname)   AS table,\n",
    "    si.size/1024/1024 AS MB\n",
    "    FROM pg_class t\n",
    "    JOIN pg_namespace n ON n.oid = t.relnamespace\n",
    "    JOIN svv_table_info si ON si.table = t.relname\n",
    "    WHERE n.nspname NOT IN ('pg_internal', 'pg_catalog', 'information_schema')\n",
    "    ORDER BY MB DESC\n",
    "    LIMIT 10;''')\n",
    "    result = cur.fetchone()\n",
    "    print(result)\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(\"Connection failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "453c1736-9bc6-45e7-92c2-2b3211a0d1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Redshift Serverless!\n",
      "Current date in Redshift: 2025-07-08\n",
      "Current user and database name>  ('admin', 'dev')\n",
      "Schemaas  None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Redshift Serverless connection details\n",
    "host = \"default-workgroup.xx.us-east-1.redshift-serverless.amazonaws.com\"\n",
    "#:5439/dev\"\n",
    "#'<your-workgroup-name>.<random>.redshift-serverless.<region>.amazonaws.com'\n",
    "\n",
    "port = 5439\n",
    "dbname = 'dev'\n",
    "user = 'admin'\n",
    "password = 'xxx'\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        host=host,\n",
    "        port=port,\n",
    "        dbname=dbname,\n",
    "        user=user,\n",
    "        password=password\n",
    "    )\n",
    "    print(\"Connected to Redshift Serverless!\")\n",
    "\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT current_date;\")\n",
    "    result = cur.fetchone()\n",
    "    print(\"Current date in Redshift:\", result[0])\n",
    "\n",
    "    cur.execute(\"SELECT current_user, current_database();\")\n",
    "    result = cur.fetchone()\n",
    "    print(\"Current user and database name> \", result)\n",
    "\n",
    "    cur.execute(\"SELECT schema_name FROM information_schema.schemata ORDER BY 1;\")\n",
    "    result = cur.fetchone()\n",
    "    print(\"Schemaas \", result)\n",
    "\n",
    "    cur.execute('''SELECT\n",
    "    trim(n.nspname)   AS schema,\n",
    "    trim(t.relname)   AS table,\n",
    "    si.size/1024/1024 AS MB\n",
    "    FROM pg_class t\n",
    "    JOIN pg_namespace n ON n.oid = t.relnamespace\n",
    "    JOIN svv_table_info si ON si.table = t.relname\n",
    "    WHERE n.nspname NOT IN ('pg_internal', 'pg_catalog', 'information_schema')\n",
    "    ORDER BY MB DESC\n",
    "    LIMIT 10;''')\n",
    "    result = cur.fetchone()\n",
    "    print(result)\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(\"Connection failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1e3a2a6-2d67-44b2-b620-5cf1ad3c43d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Redshift Serverless!\n"
     ]
    }
   ],
   "source": [
    "# 1. Simple conect and read a table\n",
    "\n",
    "def get_conn_from_redshift():\n",
    "    # Redshift Serverless connection details\n",
    "    host = \"default-workgroup.310879042055.us-east-1.redshift-serverless.amazonaws.com\"\n",
    "    #:5439/dev\"\n",
    "    #'<your-workgroup-name>.<random>.redshift-serverless.<region>.amazonaws.com'\n",
    "    \n",
    "    port = 5439\n",
    "    dbname = 'dev'\n",
    "    user = 'admin'\n",
    "    password = 'KkmAcl4526'\n",
    "    \n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=host,\n",
    "            port=port,\n",
    "            dbname=dbname,\n",
    "            user=user,\n",
    "            password=password\n",
    "        )\n",
    "        print(\"Connected to Redshift Serverless!\")\n",
    "        return conn\n",
    "    except:\n",
    "        print(\"### ERROR in connecting ####\")\n",
    "\n",
    "\n",
    "# 1. Get Connection\n",
    "conn = get_conn_from_redshift()\n",
    "\n",
    "# # 2. Read table\n",
    "# import pandas as pd\n",
    "# df = pd.read_sql(\"SELECT * FROM sales LIMIT 100\", conn)\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cda3f07f-9f50-4fbb-9e70-6682053bd68e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_conn_from_redshift' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Connected to Redshift Serverless!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m conn\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m conn = \u001b[43mget_conn_from_redshift\u001b[49m()\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 2. Create a sample table\u001b[39;00m\n\u001b[32m     31\u001b[39m DDL = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[33mCREATE TABLE IF NOT EXISTS sales (\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[33m    id      BIGINT IDENTITY,\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m \u001b[33mVALUES (\u001b[39m\u001b[33m'\u001b[39m\u001b[33mABC‑001\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, 2, 199.99), (\u001b[39m\u001b[33m'\u001b[39m\u001b[33mABC‑002\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, 1, 349.00);\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'get_conn_from_redshift' is not defined"
     ]
    }
   ],
   "source": [
    "#\n",
    "import psycopg2\n",
    "def get_conn_redshift():\n",
    "    host = \"default-workgroup.310879042055.us-east-1.redshift-serverless.amazonaws.com\"\n",
    "    #:5439/dev\"\n",
    "    #'<your-workgroup-name>.<random>.redshift-serverless.<region>.amazonaws.com'\n",
    "    \n",
    "    port = 5439\n",
    "    dbname = 'dev'\n",
    "    user = 'admin'\n",
    "    password = 'KkmAcl4526'\n",
    "    conn = psycopg2.connect(\n",
    "        host=host,\n",
    "        port=port,\n",
    "        dbname=dbname,\n",
    "        user=user,\n",
    "        password=password,\n",
    "        connect_timeout=10,\n",
    "        keepalives=1,\n",
    "        keepalives_idle=10,\n",
    "        keepalives_interval=10,\n",
    "        keepalives_count=5,\n",
    "        sslmode=\"require\",          # Redshift enforces TLS\n",
    "    )\n",
    "    print(\"✅ Connected to Redshift Serverless!\")\n",
    "    return conn\n",
    "conn = get_conn_from_redshift()\n",
    "\n",
    "\n",
    "# 2. Create a sample table\n",
    "DDL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS sales (\n",
    "    id      BIGINT IDENTITY,\n",
    "    ts      TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    sku     VARCHAR(32),\n",
    "    qty     INT,\n",
    "    price   NUMERIC(12,2)\n",
    ");\n",
    "INSERT INTO sales (sku, qty, price)\n",
    "VALUES ('ABC‑001', 2, 199.99), ('ABC‑002', 1, 349.00);\n",
    "\"\"\"\n",
    "\n",
    "cur = conn.cursor()\n",
    "resp = cur.execute(DDL)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e001729-aaf6-41c8-91f4-ac49d4eb7c1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError_",
     "evalue": "exception name : UnauthorizedException, error type : 138, message: The requested role arn:aws:iam::310879042055:role/aws-service-role/redshift.amazonaws.com/AWSServiceRoleForRedshift is not associated to cluster, should retry : 0\nDETAIL:  \n  -----------------------------------------------\n  error:  exception name : UnauthorizedException, error type : 138, message: The requested role arn:aws:iam::310879042055:role/aws-service-role/redshift.amazonaws.com/AWSServiceRoleForRedshift is not associated to cluster, should retry : 0\n  code:      30000\n  context:   \n  query:     409640[child_sequence:4]\n  location:  xen_aws_credentials_mgr.cpp:785\n  process:   padbmaster [pid=1073889539]\n  -----------------------------------------------\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInternalError_\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m COPY_CMD = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[33mCOPY sales\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33mFROM \u001b[39m\u001b[33m'\u001b[39m\u001b[33ms3://kkm2-unique-test-bucket-2025-06-26-py-1/sales.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33mIAM_ROLE \u001b[39m\u001b[33m'\u001b[39m\u001b[33marn:aws:iam::310879042055:role/aws-service-role/redshift.amazonaws.com/AWSServiceRoleForRedshift\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33mFORMAT AS PARQUET;\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mcur\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCOPY_CMD\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mInternalError_\u001b[39m: exception name : UnauthorizedException, error type : 138, message: The requested role arn:aws:iam::310879042055:role/aws-service-role/redshift.amazonaws.com/AWSServiceRoleForRedshift is not associated to cluster, should retry : 0\nDETAIL:  \n  -----------------------------------------------\n  error:  exception name : UnauthorizedException, error type : 138, message: The requested role arn:aws:iam::310879042055:role/aws-service-role/redshift.amazonaws.com/AWSServiceRoleForRedshift is not associated to cluster, should retry : 0\n  code:      30000\n  context:   \n  query:     409640[child_sequence:4]\n  location:  xen_aws_credentials_mgr.cpp:785\n  process:   padbmaster [pid=1073889539]\n  -----------------------------------------------\n\n"
     ]
    }
   ],
   "source": [
    "COPY_CMD = f\"\"\"\n",
    "COPY sales\n",
    "FROM 's3://kkm2-unique-test-bucket-2025-06-26-py-1/sales.csv'\n",
    "IAM_ROLE 'arn:aws:iam::310879042055:role/aws-service-role/redshift.amazonaws.com/AWSServiceRoleForRedshift'\n",
    "FORMAT AS PARQUET;\n",
    "\"\"\"\n",
    "cur.execute(COPY_CMD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e6ccd0-d778-4c2c-8597-d25238d26aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
