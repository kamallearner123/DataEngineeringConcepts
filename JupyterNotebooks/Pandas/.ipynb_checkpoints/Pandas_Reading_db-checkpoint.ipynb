{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data from MySQL and SQLite in Python\n",
    "\n",
    "This Jupyter Notebook demonstrates how to read data from MySQL and SQLite databases using Python, with a focus on handling large datasets efficiently. We'll use `mysql-connector-python` for MySQL and the built-in `sqlite3` module for SQLite. The notebook covers:\n",
    "- Basic database connections and queries\n",
    "- Handling large datasets with chunking and pagination\n",
    "- Connection pooling for MySQL\n",
    "- Visualizing data with matplotlib\n",
    "- Best practices for large databases\n",
    "\n",
    "## Prerequisites\n",
    "- Install required libraries: `pip install mysql-connector-python pandas matplotlib`\n",
    "- Set up MySQL and SQLite databases (see Setup section).\n",
    "- Replace placeholders (`your_username`, `your_password`) with actual MySQL credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Install Libraries\n",
    "```bash\n",
    "pip install mysql-connector-python pandas matplotlib\n",
    "```\n",
    "\n",
    "### MySQL Database Setup\n",
    "Run the following SQL commands in your MySQL client to create a sample database:\n",
    "```sql\n",
    "CREATE DATABASE sales_db;\n",
    "USE sales_db;\n",
    "CREATE TABLE sales (\n",
    "    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "    product VARCHAR(100),\n",
    "    amount DECIMAL(10, 2),\n",
    "    sale_date DATE,\n",
    "    INDEX idx_sale_date (sale_date)\n",
    ");\n",
    "```\n",
    "\n",
    "### SQLite Database Setup\n",
    "Create a SQLite database file (`sales.db`) with the following Python code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create SQLite database and table\n",
    "conn = sqlite3.connect('sales.db')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS sales (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        product TEXT,\n",
    "        amount REAL,\n",
    "        sale_date TEXT\n",
    "    )\n",
    "''')\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MySQL: Reading Data\n",
    "\n",
    "### Basic Connection and Query\n",
    "Connect to MySQL and fetch a small subset of data from the `sales` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "\n",
    "try:\n",
    "    connection = mysql.connector.connect(\n",
    "        host='localhost',\n",
    "        database='sales_db',\n",
    "        user='your_username',\n",
    "        password='your_password'\n",
    "    )\n",
    "    query = \"SELECT * FROM sales WHERE sale_date >= '2023-01-01' LIMIT 10\"\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(query)\n",
    "    rows = cursor.fetchall()\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "except Error as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    if connection.is_connected():\n",
    "        cursor.close()\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Large Datasets\n",
    "Use chunking with `fetchmany` and an unbuffered cursor to process large datasets efficiently. Also, implement pagination with `LIMIT` and `OFFSET`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_in_chunks(query, chunk_size=1000):\n",
    "    try:\n",
    "        connection = mysql.connector.connect(\n",
    "            host='localhost',\n",
    "            database='sales_db',\n",
    "            user='your_username',\n",
    "            password='your_password'\n",
    "        )\n",
    "        cursor = connection.cursor(buffered=False)  # Stream results\n",
    "        cursor.execute(query)\n",
    "        while True:\n",
    "            chunk = cursor.fetchmany(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            df = pd.DataFrame(chunk, columns=[desc[0] for desc in cursor.description])\n",
    "            yield df\n",
    "    except Error as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "\n",
    "# Example: Process large dataset\n",
    "query = \"SELECT * FROM sales\"\n",
    "for df_chunk in fetch_in_chunks(query, chunk_size=1000):\n",
    "    print(df_chunk.head())  # Process chunk (e.g., save to CSV, analyze)\n",
    "\n",
    "# Pagination Example\n",
    "def fetch_paginated(page, page_size=1000):\n",
    "    offset = (page - 1) * page_size\n",
    "    query = f\"SELECT * FROM sales LIMIT {page_size} OFFSET {offset}\"\n",
    "    connection = mysql.connector.connect(\n",
    "        host='localhost',\n",
    "        database='sales_db',\n",
    "        user='your_username',\n",
    "        password='your_password'\n",
    "    )\n",
    "    df = pd.read_sql(query, connection)\n",
    "    connection.close()\n",
    "    return df\n",
    "\n",
    "# Fetch page 1\n",
    "df_page = fetch_paginated(page=1)\n",
    "print(df_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection Pooling\n",
    "Use connection pooling for efficient handling of multiple queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mysql.connector.pooling import MySQLConnectionPool\n",
    "\n",
    "pool_config = {\n",
    "    \"pool_name\": \"mypool\",\n",
    "    \"pool_size\": 5,\n",
    "    \"host\": \"localhost\",\n",
    "    \"database\": \"sales_db\",\n",
    "    \"user\": \"your_username\",\n",
    "    \"password\": \"your_password\"\n",
    "}\n",
    "db_pool = MySQLConnectionPool(**pool_config)\n",
    "\n",
    "# Fetch data using pooled connection\n",
    "connection = db_pool.get_connection()\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"SELECT COUNT(*) FROM sales\")\n",
    "count = cursor.fetchone()[0]\n",
    "print(f\"Total rows: {count}\")\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQLite: Reading Data\n",
    "\n",
    "### Basic Connection and Query\n",
    "Connect to SQLite and fetch data from the `sales` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "try:\n",
    "    connection = sqlite3.connect('sales.db')\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"SELECT * FROM sales WHERE sale_date >= '2023-01-01' LIMIT 10\")\n",
    "    rows = cursor.fetchall()\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "except sqlite3.Error as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    cursor.close()\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Large Datasets\n",
    "Use `fetchmany` to process large datasets in chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_in_chunks(query, chunk_size=1000):\n",
    "    try:\n",
    "        connection = sqlite3.connect('sales.db')\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query)\n",
    "        while True:\n",
    "            chunk = cursor.fetchmany(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            df = pd.DataFrame(chunk, columns=[desc[0] for desc in cursor.description])\n",
    "            yield df\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "\n",
    "# Example: Process large dataset\n",
    "query = \"SELECT * FROM sales\"\n",
    "for df_chunk in fetch_in_chunks(query, chunk_size=1000):\n",
    "    print(df_chunk.head())  # Process chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Visualize sample data (e.g., row counts) from MySQL and SQLite databases using `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data (replace with actual row counts)\n",
    "databases = ['MySQL', 'SQLite']\n",
    "row_counts = [1000000, 500000]  # Example counts\n",
    "\n",
    "plt.bar(databases, row_counts, color=['#1E90FF', '#FF6347'])\n",
    "plt.title('Database Row Counts')\n",
    "plt.xlabel('Database')\n",
    "plt.ylabel('Rows')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Large Databases\n",
    "\n",
    "- **Indexing**: Create indexes on frequently queried columns (e.g., `sale_date`).\n",
    "- **_CHUNKING**: Use `fetchmany` or pandas chunking to process large datasets.\n",
    "- **Pagination**: Implement `LIMIT` and `OFFSET` for controlled data retrieval.\n",
    "- **Connection Pooling**: Use for MySQL to manage multiple connections efficiently.\n",
    "- **Error Handling**: Always include try-except blocks for robust code.\n",
    "- **Optimize Queries**: Avoid `SELECT *`; specify required columns.\n",
    "\n",
    "## Notes\n",
    "- Replace placeholders (`your_username`, `your_password`) with actual MySQL credentials.\n",
    "- For large datasets, adjust `chunk_size` based on available memory.\n",
    "- Store credentials securely (e.g., in environment variables) for production use.\n",
    "- Consider using `SQLAlchemy` for more advanced database operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
