{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science as a Detective Story: Unraveling the \"Lost Customers\" Mystery with Pandas\n",
    "\n",
    "Welcome to the world of Data Science! Imagine you're a detective, and your main tool is **Pandas**, Python's powerhouse library for data manipulation and analysis.\n",
    "\n",
    "We'll explore how Pandas helps us solve real-world business mysteries, like identifying why an online store might be losing customers. This journey will walk through the typical **Data Science workflow**, showing where Pandas shines at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mystery: GadgetGrove's \"Lost Customers\"\n",
    "\n",
    "**Scenario:** You work for \"GadgetGrove,\" a popular online electronics store. The marketing team is worried; they feel they're losing customers, but they don't know *why* or *who*. They need to understand customer behavior better to launch effective retention campaigns.\n",
    "\n",
    "**Your Data Science Mission:** Use GadgetGrove's past sales data to identify potential \"lost\" customers (those who haven't purchased in a while) and understand their characteristics. This information will help the marketing team bring them back!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Gathering the Clues (Data Acquisition)\n",
    "\n",
    "Before we can solve any mystery, we need the raw evidence. In data science, this means loading our data into a **Pandas DataFrame** â€“ which is like a super-powered spreadsheet in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # Used later for introducing NaNs\n",
    "import matplotlib.pyplot as plt # For visualizations\n",
    "import seaborn as sns # For visualizations\n",
    "\n",
    "# For demonstration, we'll create a small dummy DataFrame that resembles sales data.\n",
    "# In a real scenario, you'd use pd.read_csv('sales_data.csv') or pd.read_excel().\n",
    "data = {\n",
    "    'InvoiceID': [1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012],\n",
    "    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST001', 'CUST002', 'CUST004', 'CUST003', 'CUST005', 'CUST001', 'CUST002', 'CUST006'],\n",
    "    'ProductID': ['P001', 'P005', 'P002', 'P001', 'P003', 'P005', 'P004', 'P006', 'P001', 'P002', 'P007', 'P008'],\n",
    "    'Quantity': [2, 1, 3, 1, 1, 2, 1, 1, 3, 2, 1, 1],\n",
    "    'Price': [10.50, 50.00, 5.00, 10.50, 15.00, 50.00, 25.00, 40.00, 10.50, 5.00, 30.00, 60.00],\n",
    "    'OrderDate': ['2023-01-05', '2023-01-06', '2023-01-10', '2023-02-01', '2023-03-01', '2023-04-15', '2023-05-01', '2024-01-20', '2024-02-10', '2024-05-25', '2024-06-01', '2023-08-15'],\n",
    "    'City': ['NYC', 'LA', 'NYC', 'CHI', 'NYC', 'LA', 'SF', 'CHI', 'NYC', 'NYC', 'LA', 'HOU']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"### Initial Data (first 5 rows): ###\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n### Data Info (Column types and non-null counts): ###\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Cleaning the Clues (Data Preprocessing)\n",
    "\n",
    "Raw data is rarely perfect. It might have incorrect formats, missing values, or duplicates. Just like a detective cleans fingerprints, we clean our data to ensure our analysis is accurate.\n",
    "\n",
    "**Pandas' Role:** Essential for fixing data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1: The 'OrderDate' column is currently an 'object' (string). \n",
    "# We can't do date calculations until it's a proper datetime object.\n",
    "# Pandas Tool: pd.to_datetime() - Converts strings to datetime objects.\n",
    "df['OrderDate'] = pd.to_datetime(df['OrderDate'])\n",
    "print(\"### After converting 'OrderDate' to datetime: ###\")\n",
    "df.info()\n",
    "\n",
    "# Problem 2: What if some CustomerIDs are missing? We can't track 'lost' customers if we don't know who they are.\n",
    "# Let's artificially introduce a missing CustomerID for demonstration.\n",
    "df.loc[df['CustomerID'] == 'CUST005', 'CustomerID'] = np.nan # Set CUST005 to NaN\n",
    "\n",
    "print(\"\\n### Data with artificial missing CustomerID (counts per column): ###\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Detective Action: Since a missing CustomerID means we can't identify the customer,\n",
    "# we'll drop those rows. (Other strategies include filling with a placeholder).\n",
    "# Pandas Tool: df.dropna() - Removes rows/columns with missing values. \n",
    "# 'subset' ensures we only drop if 'CustomerID' is missing.\n",
    "df_cleaned = df.dropna(subset=['CustomerID']).copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "print(\"\\n### After dropping rows with missing CustomerID: ###\")\n",
    "print(df_cleaned.isnull().sum())\n",
    "print(df_cleaned)\n",
    "\n",
    "# Detective Action: Calculate 'TotalPrice' for each order item, which is crucial for spending analysis.\n",
    "# Pandas Tool: Basic arithmetic operations on columns (Series).\n",
    "df_cleaned['TotalPrice'] = df_cleaned['Quantity'] * df_cleaned['Price']\n",
    "print(\"\\n### After adding 'TotalPrice' column: ###\")\n",
    "print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Exploring the Clues (Exploratory Data Analysis - EDA)\n",
    "\n",
    "This is where the real detective work happens! We'll use Pandas to summarize, aggregate, and visualize our data to uncover hidden patterns and answer our central question: \"Who are the lost customers and what are they like?\"\n",
    "\n",
    "**Pandas' Role:** The engine for data summarization, aggregation, filtering, and preparing data for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll assume today's date for our analysis. \n",
    "# In a real application, you'd use pd.to_datetime('today').\n",
    "current_date = pd.to_datetime('2024-06-05') \n",
    "print(f\"### Current Analysis Date: {current_date.strftime('%Y-%m-%d')} ###\")\n",
    "\n",
    "# Step 1: Find the *last purchase date* for each customer.\n",
    "# Detective Action: To know who's 'lost', we need to know when they last bought something.\n",
    "# Pandas Tool: .groupby() + .max() - Groups data by CustomerID and finds the latest OrderDate for each group.\n",
    "last_purchase_dates = df_cleaned.groupby('CustomerID')['OrderDate'].max().reset_index()\n",
    "last_purchase_dates.columns = ['CustomerID', 'LastPurchaseDate'] # Rename column for clarity\n",
    "print(\"\\n### Last Purchase Dates per Customer: ###\")\n",
    "print(last_purchase_dates)\n",
    "\n",
    "# Step 2: Calculate 'Recency' - how many days since their last purchase?\n",
    "# Detective Action: The longer the time since their last purchase, the 'lost-er' they are.\n",
    "# Pandas Tool: Subtracting datetime objects yields a timedelta, then use .dt.days.\n",
    "last_purchase_dates['Recency'] = (current_date - last_purchase_dates['LastPurchaseDate']).dt.days\n",
    "print(\"\\n### Customer Recency (days since last purchase, sorted oldest first): ###\")\n",
    "print(last_purchase_dates.sort_values(by='Recency', ascending=False))\n",
    "\n",
    "# Step 3: Identify \"Lost\" Customers\n",
    "# Detective Action: Let's define 'lost' as no purchase in over 90 days (approx. 3 months).\n",
    "# Pandas Tool: Boolean filtering - Selecting rows based on a condition.\n",
    "lost_customers_df = last_purchase_dates[last_purchase_dates['Recency'] > 90]\n",
    "print(f\"\\n### Identified {len(lost_customers_df)} 'Lost' Customers (no purchase in > 90 days): ###\")\n",
    "print(lost_customers_df.sort_values(by='Recency', ascending=False))\n",
    "\n",
    "# Step 4: Characterize Lost Customers (e.g., their average spending, most common city)\n",
    "# Detective Action: What do these 'lost' customers have in common? This helps us understand their profile.\n",
    "# Pandas Tool: .merge() to combine data, then .groupby() and .agg() for summary statistics.\n",
    "\n",
    "# First, merge the 'lost_customers_df' back with the cleaned sales data to get their transaction details.\n",
    "lost_customers_details = pd.merge(lost_customers_df, df_cleaned, on='CustomerID', how='left')\n",
    "print(\"\\n### Transaction Details of Lost Customers (first 5 rows): ###\")\n",
    "print(lost_customers_details.head())\n",
    "\n",
    "# Now, summarize characteristics of these specific lost customers.\n",
    "lost_customer_summary = lost_customers_details.groupby('CustomerID').agg(\n",
    "    Avg_Spent_Per_Order=('TotalPrice', 'mean'),\n",
    "    Total_Orders=('InvoiceID', 'nunique'), # Count unique invoices for total orders\n",
    "    Most_Frequent_City=('City', lambda x: x.mode()[0] if not x.empty else 'N/A') # Get the most frequent city\n",
    ").reset_index()\n",
    "print(\"\\n### Summary of Lost Customers: ###\")\n",
    "print(lost_customer_summary.sort_values(by='Recency', ascending=False)) # Add Recency back for full context\n",
    "\n",
    "# Add Recency back to the summary for a complete view\n",
    "lost_customer_summary = pd.merge(lost_customer_summary, lost_customers_df[['CustomerID', 'Recency']], on='CustomerID', how='left')\n",
    "print(\"\\n### Comprehensive Summary of Lost Customers (with Recency): ###\")\n",
    "print(lost_customer_summary.sort_values(by='Recency', ascending=False))\n",
    "\n",
    "# Visualizing the Findings\n",
    "print(\"\\n### Generating Visualizations... ###\")\n",
    "\n",
    "# Visualization 1: Distribution of lost customers by city\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(y='Most_Frequent_City', data=lost_customer_summary, \n",
    "              order=lost_customer_summary['Most_Frequent_City'].value_counts().index, \n",
    "              palette='viridis')\n",
    "plt.title('Distribution of Lost Customers by City')\n",
    "plt.xlabel('Number of Lost Customers')\n",
    "plt.ylabel('City')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Visualization 2: Average spending of lost customers (who had multiple orders)\n",
    "# Filter for customers with at least one order to show meaningful average spent\n",
    "if not lost_customer_summary[lost_customer_summary['Total_Orders'] > 0].empty:\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.barplot(x='CustomerID', y='Avg_Spent_Per_Order', \n",
    "                data=lost_customer_summary.sort_values(by='Avg_Spent_Per_Order', ascending=False),\n",
    "                palette='magma')\n",
    "    plt.title('Average Spending per Order of Identified Lost Customers')\n",
    "    plt.xlabel('Customer ID')\n",
    "    plt.ylabel('Average Spent per Order ($)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No lost customers with recorded orders to visualize average spending.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Unveiling the Truth (Insights & Communication)\n",
    "\n",
    "The data doesn't just sit there; it tells a story! As data scientists, we synthesize our findings into actionable insights for the business.\n",
    "\n",
    "**Detective Conclusion:**\n",
    "* \"We've identified **`CUST003`** and **`CUST006`** as our primary 'lost' customers, having not purchased in **`135`** and **`295`** days respectively. `CUST004` is also nearing 'lost' status.\"\n",
    "* \"A significant portion of these lost customers are from **`CHI`** and **`HOU`**.\"\n",
    "* \"While some lost customers had high average spending per order, it's their **recency** that is the key indicator of them being 'lost'.\"\n",
    "\n",
    "**Actionable Insights for GadgetGrove's Marketing Team:**\n",
    "* **Targeted Campaigns:** Launch specific re-engagement campaigns (e.g., personalized discounts, new product alerts) to `CUST003` and `CUST006`.\n",
    "* **Location-Based Strategy:** Investigate why customers from **Chicago (`CHI`)** and **Houston (`HOU`)** might be disengaging. Is there a new local competitor? Are delivery options worse there? Are they not seeing relevant ads?\n",
    "* **Proactive Retention:** Monitor customers like `CUST004` (at 35 days recency) more closely and intervene earlier before they become fully 'lost'.\n",
    "\n",
    "**Pandas' Role:** Pandas provided the structured data, performed all the necessary calculations (recency, spending summaries), filtered the relevant customers, and aggregated the data for clear insights and visualizations. It's the backbone of turning raw transaction logs into strategic business intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Pandas: What's Next in the Data Science Journey?\n",
    "\n",
    "This investigation used Pandas to **clean and explore** the data. In a full data science project, the next steps often involve:\n",
    "\n",
    "* **Modeling:** Building machine learning models to *predict* which customers are likely to become lost *in the future*.\n",
    "* **Deployment:** Integrating these insights and predictions into business operations (e.g., automated marketing systems).\n",
    "\n",
    "Pandas is crucial for preparing the data for these advanced steps, often creating the very **features** (like Recency, Frequency, Monetary value) that machine learning models learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn to be a Data Detective!\n",
    "\n",
    "The best way to learn Pandas and data science is by doing. Find a dataset (Kaggle is a great resource!), come up with a question, and use Pandas to uncover the answers. Every dataset is a new mystery waiting to be solved!\n",
    "\n",
    "What other business questions do you think could be answered by analyzing this sales data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
