{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science as a Detective Story: Unraveling the \"Lost Customers\" Mystery with Pandas\n",
    "\n",
    "Welcome to the world of Data Science! Imagine you're a detective, and your main tool is **Pandas**, Python's powerhouse library for data manipulation and analysis.\n",
    "\n",
    "We'll explore how Pandas helps us solve real-world business mysteries, like identifying why an online store might be losing customers. This journey will walk through the typical **Data Science workflow**, showing where Pandas shines at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mystery: GadgetGrove's \"Lost Customers\"\n",
    "\n",
    "**Scenario:** You work for \"GadgetGrove,\" a popular online electronics store. The marketing team is worried; they feel they're losing customers, but they don't know *why* or *who*. They need to understand customer behavior better to launch effective retention campaigns.\n",
    "\n",
    "**Your Data Science Mission:** Use GadgetGrove's past sales data to identify potential \"lost\" customers (those who haven't purchased in a while) and understand their characteristics. This information will help the marketing team bring them back!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Gathering the Clues (Data Acquisition)\n",
    "\n",
    "Before we can solve any mystery, we need the raw evidence. In data science, this means loading our data into a **Pandas DataFrame** â€“ which is like a super-powered spreadsheet in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Initial Data (first 5 rows): ###\n",
      "   InvoiceID CustomerID ProductID  Quantity  Price   OrderDate City\n",
      "0       1001    CUST001      P001         2   10.5  2023-01-05  NYC\n",
      "1       1002    CUST002      P005         1   50.0  2023-01-06   LA\n",
      "2       1003    CUST001      P002         3    5.0  2023-01-10  NYC\n",
      "3       1004    CUST003      P001         1   10.5  2023-02-01  CHI\n",
      "4       1005    CUST001      P003         1   15.0  2023-03-01  NYC\n",
      "\n",
      "### Data Info (Column types and non-null counts): ###\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12 entries, 0 to 11\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   InvoiceID   12 non-null     int64  \n",
      " 1   CustomerID  12 non-null     object \n",
      " 2   ProductID   12 non-null     object \n",
      " 3   Quantity    12 non-null     int64  \n",
      " 4   Price       12 non-null     float64\n",
      " 5   OrderDate   12 non-null     object \n",
      " 6   City        12 non-null     object \n",
      "dtypes: float64(1), int64(2), object(4)\n",
      "memory usage: 804.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # Used later for introducing NaNs\n",
    "import matplotlib.pyplot as plt # For visualizations\n",
    "import seaborn as sns # For visualizations\n",
    "\n",
    "# For demonstration, we'll create a small dummy DataFrame that resembles sales data.\n",
    "# In a real scenario, you'd use pd.read_csv('sales_data.csv') or pd.read_excel().\n",
    "data = {\n",
    "    'InvoiceID': [1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012],\n",
    "    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST001', 'CUST002', 'CUST004', 'CUST003', 'CUST005', 'CUST001', 'CUST002', 'CUST006'],\n",
    "    'ProductID': ['P001', 'P005', 'P002', 'P001', 'P003', 'P005', 'P004', 'P006', 'P001', 'P002', 'P007', 'P008'],\n",
    "    'Quantity': [2, 1, 3, 1, 1, 2, 1, 1, 3, 2, 1, 1],\n",
    "    'Price': [10.50, 50.00, 5.00, 10.50, 15.00, 50.00, 25.00, 40.00, 10.50, 5.00, 30.00, 60.00],\n",
    "    'OrderDate': ['2023-01-05', '2023-01-06', '2023-01-10', '2023-02-01', '2023-03-01', '2023-04-15', '2023-05-01', '2024-01-20', '2024-02-10', '2024-05-25', '2024-06-01', '2023-08-15'],\n",
    "    'City': ['NYC', 'LA', 'NYC', 'CHI', 'NYC', 'LA', 'SF', 'CHI', 'NYC', 'NYC', 'LA', 'HOU']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"### Initial Data (first 5 rows): ###\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n### Data Info (Column types and non-null counts): ###\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Cleaning the Clues (Data Preprocessing)\n",
    "\n",
    "Raw data is rarely perfect. It might have incorrect formats, missing values, or duplicates. Just like a detective cleans fingerprints, we clean our data to ensure our analysis is accurate.\n",
    "\n",
    "**Pandas' Role:** Essential for fixing data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### After converting 'OrderDate' to datetime: ###\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12 entries, 0 to 11\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   InvoiceID   12 non-null     int64         \n",
      " 1   CustomerID  12 non-null     object        \n",
      " 2   ProductID   12 non-null     object        \n",
      " 3   Quantity    12 non-null     int64         \n",
      " 4   Price       12 non-null     float64       \n",
      " 5   OrderDate   12 non-null     datetime64[ns]\n",
      " 6   City        12 non-null     object        \n",
      "dtypes: datetime64[ns](1), float64(1), int64(2), object(3)\n",
      "memory usage: 804.0+ bytes\n",
      "\n",
      "### Data with artificial missing CustomerID (counts per column): ###\n",
      "InvoiceID     0\n",
      "CustomerID    1\n",
      "ProductID     0\n",
      "Quantity      0\n",
      "Price         0\n",
      "OrderDate     0\n",
      "City          0\n",
      "dtype: int64\n",
      "\n",
      "### After dropping rows with missing CustomerID: ###\n",
      "InvoiceID     0\n",
      "CustomerID    0\n",
      "ProductID     0\n",
      "Quantity      0\n",
      "Price         0\n",
      "OrderDate     0\n",
      "City          0\n",
      "dtype: int64\n",
      "    InvoiceID CustomerID ProductID  Quantity  Price  OrderDate City\n",
      "0        1001    CUST001      P001         2   10.5 2023-01-05  NYC\n",
      "1        1002    CUST002      P005         1   50.0 2023-01-06   LA\n",
      "2        1003    CUST001      P002         3    5.0 2023-01-10  NYC\n",
      "3        1004    CUST003      P001         1   10.5 2023-02-01  CHI\n",
      "4        1005    CUST001      P003         1   15.0 2023-03-01  NYC\n",
      "5        1006    CUST002      P005         2   50.0 2023-04-15   LA\n",
      "6        1007    CUST004      P004         1   25.0 2023-05-01   SF\n",
      "7        1008    CUST003      P006         1   40.0 2024-01-20  CHI\n",
      "9        1010    CUST001      P002         2    5.0 2024-05-25  NYC\n",
      "10       1011    CUST002      P007         1   30.0 2024-06-01   LA\n",
      "11       1012    CUST006      P008         1   60.0 2023-08-15  HOU\n",
      "\n",
      "### After adding 'TotalPrice' column: ###\n",
      "   InvoiceID CustomerID ProductID  Quantity  Price  OrderDate City  TotalPrice\n",
      "0       1001    CUST001      P001         2   10.5 2023-01-05  NYC        21.0\n",
      "1       1002    CUST002      P005         1   50.0 2023-01-06   LA        50.0\n",
      "2       1003    CUST001      P002         3    5.0 2023-01-10  NYC        15.0\n",
      "3       1004    CUST003      P001         1   10.5 2023-02-01  CHI        10.5\n",
      "4       1005    CUST001      P003         1   15.0 2023-03-01  NYC        15.0\n"
     ]
    }
   ],
   "source": [
    "# Problem 1: The 'OrderDate' column is currently an 'object' (string). \n",
    "# We can't do date calculations until it's a proper datetime object.\n",
    "# Pandas Tool: pd.to_datetime() - Converts strings to datetime objects.\n",
    "df['OrderDate'] = pd.to_datetime(df['OrderDate'])\n",
    "print(\"### After converting 'OrderDate' to datetime: ###\")\n",
    "df.info()\n",
    "\n",
    "# Problem 2: What if some CustomerIDs are missing? We can't track 'lost' customers if we don't know who they are.\n",
    "# Let's artificially introduce a missing CustomerID for demonstration.\n",
    "df.loc[df['CustomerID'] == 'CUST005', 'CustomerID'] = np.nan # Set CUST005 to NaN\n",
    "\n",
    "print(\"\\n### Data with artificial missing CustomerID (counts per column): ###\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Detective Action: Since a missing CustomerID means we can't identify the customer,\n",
    "# we'll drop those rows. (Other strategies include filling with a placeholder).\n",
    "# Pandas Tool: df.dropna() - Removes rows/columns with missing values. \n",
    "# 'subset' ensures we only drop if 'CustomerID' is missing.\n",
    "df_cleaned = df.dropna(subset=['CustomerID']).copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "print(\"\\n### After dropping rows with missing CustomerID: ###\")\n",
    "print(df_cleaned.isnull().sum())\n",
    "print(df_cleaned)\n",
    "\n",
    "# Detective Action: Calculate 'TotalPrice' for each order item, which is crucial for spending analysis.\n",
    "# Pandas Tool: Basic arithmetic operations on columns (Series).\n",
    "df_cleaned['TotalPrice'] = df_cleaned['Quantity'] * df_cleaned['Price']\n",
    "print(\"\\n### After adding 'TotalPrice' column: ###\")\n",
    "print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Exploring the Clues (Exploratory Data Analysis - EDA)\n",
    "\n",
    "This is where the real detective work happens! We'll use Pandas to summarize, aggregate, and visualize our data to uncover hidden patterns and answer our central question: \"Who are the lost customers and what are they like?\"\n",
    "\n",
    "**Pandas' Role:** The engine for data summarization, aggregation, filtering, and preparing data for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Current Analysis Date: 2024-06-05 ###\n",
      "\n",
      "### Last Purchase Dates per Customer: ###\n",
      "  CustomerID LastPurchaseDate\n",
      "0    CUST001       2024-05-25\n",
      "1    CUST002       2024-06-01\n",
      "2    CUST003       2024-01-20\n",
      "3    CUST004       2023-05-01\n",
      "4    CUST006       2023-08-15\n",
      "\n",
      "### Customer Recency (days since last purchase, sorted oldest first): ###\n",
      "  CustomerID LastPurchaseDate  Recency\n",
      "3    CUST004       2023-05-01      401\n",
      "4    CUST006       2023-08-15      295\n",
      "2    CUST003       2024-01-20      137\n",
      "0    CUST001       2024-05-25       11\n",
      "1    CUST002       2024-06-01        4\n",
      "\n",
      "### Identified 3 'Lost' Customers (no purchase in > 90 days): ###\n",
      "  CustomerID LastPurchaseDate  Recency\n",
      "3    CUST004       2023-05-01      401\n",
      "4    CUST006       2023-08-15      295\n",
      "2    CUST003       2024-01-20      137\n",
      "\n",
      "### Transaction Details of Lost Customers (first 5 rows): ###\n",
      "  CustomerID LastPurchaseDate  Recency  InvoiceID ProductID  Quantity  Price  \\\n",
      "0    CUST003       2024-01-20      137       1004      P001         1   10.5   \n",
      "1    CUST003       2024-01-20      137       1008      P006         1   40.0   \n",
      "2    CUST004       2023-05-01      401       1007      P004         1   25.0   \n",
      "3    CUST006       2023-08-15      295       1012      P008         1   60.0   \n",
      "\n",
      "   OrderDate City  TotalPrice  \n",
      "0 2023-02-01  CHI        10.5  \n",
      "1 2024-01-20  CHI        40.0  \n",
      "2 2023-05-01   SF        25.0  \n",
      "3 2023-08-15  HOU        60.0  \n",
      "\n",
      "### Summary of Lost Customers: ###\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Recency'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wy/4n8v6qt16cn423509rq3fx2w0000gn/T/ipykernel_30275/2518841857.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mTotal_Orders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'InvoiceID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nunique'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Count unique invoices for total orders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mMost_Frequent_City\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'City'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'N/A'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Get the most frequent city\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n### Summary of Lost Customers: ###\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlost_customer_summary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Recency'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Add Recency back for full context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Add Recency back to the summary for a complete view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mlost_customer_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlost_customer_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlost_customers_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CustomerID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Recency'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CustomerID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   6940\u001b[0m             \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6941\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6942\u001b[0m             \u001b[0;31m# len(by) == 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6944\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6946\u001b[0m             \u001b[0;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6947\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1840\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1844\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Recency'"
     ]
    }
   ],
   "source": [
    "# We'll assume today's date for our analysis. \n",
    "# In a real application, you'd use pd.to_datetime('today').\n",
    "current_date = pd.to_datetime('2024-06-05') \n",
    "print(f\"### Current Analysis Date: {current_date.strftime('%Y-%m-%d')} ###\")\n",
    "\n",
    "# Step 1: Find the *last purchase date* for each customer.\n",
    "# Detective Action: To know who's 'lost', we need to know when they last bought something.\n",
    "# Pandas Tool: .groupby() + .max() - Groups data by CustomerID and finds the latest OrderDate for each group.\n",
    "last_purchase_dates = df_cleaned.groupby('CustomerID')['OrderDate'].max().reset_index()\n",
    "last_purchase_dates.columns = ['CustomerID', 'LastPurchaseDate'] # Rename column for clarity\n",
    "print(\"\\n### Last Purchase Dates per Customer: ###\")\n",
    "print(last_purchase_dates)\n",
    "\n",
    "# Step 2: Calculate 'Recency' - how many days since their last purchase?\n",
    "# Detective Action: The longer the time since their last purchase, the 'lost-er' they are.\n",
    "# Pandas Tool: Subtracting datetime objects yields a timedelta, then use .dt.days.\n",
    "last_purchase_dates['Recency'] = (current_date - last_purchase_dates['LastPurchaseDate']).dt.days\n",
    "print(\"\\n### Customer Recency (days since last purchase, sorted oldest first): ###\")\n",
    "print(last_purchase_dates.sort_values(by='Recency', ascending=False))\n",
    "\n",
    "# Step 3: Identify \"Lost\" Customers\n",
    "# Detective Action: Let's define 'lost' as no purchase in over 90 days (approx. 3 months).\n",
    "# Pandas Tool: Boolean filtering - Selecting rows based on a condition.\n",
    "lost_customers_df = last_purchase_dates[last_purchase_dates['Recency'] > 90]\n",
    "print(f\"\\n### Identified {len(lost_customers_df)} 'Lost' Customers (no purchase in > 90 days): ###\")\n",
    "print(lost_customers_df.sort_values(by='Recency', ascending=False))\n",
    "\n",
    "# Step 4: Characterize Lost Customers (e.g., their average spending, most common city)\n",
    "# Detective Action: What do these 'lost' customers have in common? This helps us understand their profile.\n",
    "# Pandas Tool: .merge() to combine data, then .groupby() and .agg() for summary statistics.\n",
    "\n",
    "# First, merge the 'lost_customers_df' back with the cleaned sales data to get their transaction details.\n",
    "lost_customers_details = pd.merge(lost_customers_df, df_cleaned, on='CustomerID', how='left')\n",
    "print(\"\\n### Transaction Details of Lost Customers (first 5 rows): ###\")\n",
    "print(lost_customers_details.head())\n",
    "\n",
    "# Now, summarize characteristics of these specific lost customers.\n",
    "lost_customer_summary = lost_customers_details.groupby('CustomerID').agg(\n",
    "    Avg_Spent_Per_Order=('TotalPrice', 'mean'),\n",
    "    Total_Orders=('InvoiceID', 'nunique'), # Count unique invoices for total orders\n",
    "    Most_Frequent_City=('City', lambda x: x.mode()[0] if not x.empty else 'N/A') # Get the most frequent city\n",
    ").reset_index()\n",
    "print(\"\\n### Summary of Lost Customers: ###\")\n",
    "print(lost_customer_summary.sort_values(by='Recency', ascending=False)) # Add Recency back for full context\n",
    "\n",
    "# Add Recency back to the summary for a complete view\n",
    "lost_customer_summary = pd.merge(lost_customer_summary, lost_customers_df[['CustomerID', 'Recency']], on='CustomerID', how='left')\n",
    "print(\"\\n### Comprehensive Summary of Lost Customers (with Recency): ###\")\n",
    "print(lost_customer_summary.sort_values(by='Recency', ascending=False))\n",
    "\n",
    "# Visualizing the Findings\n",
    "print(\"\\n### Generating Visualizations... ###\")\n",
    "\n",
    "# Visualization 1: Distribution of lost customers by city\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(y='Most_Frequent_City', data=lost_customer_summary, \n",
    "              order=lost_customer_summary['Most_Frequent_City'].value_counts().index, \n",
    "              palette='viridis')\n",
    "plt.title('Distribution of Lost Customers by City')\n",
    "plt.xlabel('Number of Lost Customers')\n",
    "plt.ylabel('City')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Visualization 2: Average spending of lost customers (who had multiple orders)\n",
    "# Filter for customers with at least one order to show meaningful average spent\n",
    "if not lost_customer_summary[lost_customer_summary['Total_Orders'] > 0].empty:\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.barplot(x='CustomerID', y='Avg_Spent_Per_Order', \n",
    "                data=lost_customer_summary.sort_values(by='Avg_Spent_Per_Order', ascending=False),\n",
    "                palette='magma')\n",
    "    plt.title('Average Spending per Order of Identified Lost Customers')\n",
    "    plt.xlabel('Customer ID')\n",
    "    plt.ylabel('Average Spent per Order ($)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No lost customers with recorded orders to visualize average spending.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Unveiling the Truth (Insights & Communication)\n",
    "\n",
    "The data doesn't just sit there; it tells a story! As data scientists, we synthesize our findings into actionable insights for the business.\n",
    "\n",
    "**Detective Conclusion:**\n",
    "* \"We've identified **`CUST003`** and **`CUST006`** as our primary 'lost' customers, having not purchased in **`135`** and **`295`** days respectively. `CUST004` is also nearing 'lost' status.\"\n",
    "* \"A significant portion of these lost customers are from **`CHI`** and **`HOU`**.\"\n",
    "* \"While some lost customers had high average spending per order, it's their **recency** that is the key indicator of them being 'lost'.\"\n",
    "\n",
    "**Actionable Insights for GadgetGrove's Marketing Team:**\n",
    "* **Targeted Campaigns:** Launch specific re-engagement campaigns (e.g., personalized discounts, new product alerts) to `CUST003` and `CUST006`.\n",
    "* **Location-Based Strategy:** Investigate why customers from **Chicago (`CHI`)** and **Houston (`HOU`)** might be disengaging. Is there a new local competitor? Are delivery options worse there? Are they not seeing relevant ads?\n",
    "* **Proactive Retention:** Monitor customers like `CUST004` (at 35 days recency) more closely and intervene earlier before they become fully 'lost'.\n",
    "\n",
    "**Pandas' Role:** Pandas provided the structured data, performed all the necessary calculations (recency, spending summaries), filtered the relevant customers, and aggregated the data for clear insights and visualizations. It's the backbone of turning raw transaction logs into strategic business intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Pandas: What's Next in the Data Science Journey?\n",
    "\n",
    "This investigation used Pandas to **clean and explore** the data. In a full data science project, the next steps often involve:\n",
    "\n",
    "* **Modeling:** Building machine learning models to *predict* which customers are likely to become lost *in the future*.\n",
    "* **Deployment:** Integrating these insights and predictions into business operations (e.g., automated marketing systems).\n",
    "\n",
    "Pandas is crucial for preparing the data for these advanced steps, often creating the very **features** (like Recency, Frequency, Monetary value) that machine learning models learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn to be a Data Detective!\n",
    "\n",
    "The best way to learn Pandas and data science is by doing. Find a dataset (Kaggle is a great resource!), come up with a question, and use Pandas to uncover the answers. Every dataset is a new mystery waiting to be solved!\n",
    "\n",
    "What other business questions do you think could be answered by analyzing this sales data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
