{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Writing Data with Python and Pandas (Jupyter Notebook Ready)\n",
    "\n",
    "This notebook provides a concise guide on how to read data from, and write data to, common data sources using Python's `pandas` library. We'll cover CSV, Excel, SQLite databases, and interacting with Web APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Imports for this Notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import sqlite3\n",
    "import os # For file operations\n",
    "\n",
    "# Optional: For better plot visualization in Jupyter\n",
    "# %matplotlib inline \n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CSV (Comma Separated Values) Files\n",
    "\n",
    "CSV files are plain text files that store tabular data, with values separated by commas (or other delimiters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from CSV\n",
    "\n",
    "We'll use a public CSV file for demonstration. Note: If you're using a local file, ensure it's in the same directory as your notebook or provide the full path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Source URL (Example: California Housing Dataset)\n",
    "csv_url = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df_csv = pd.read_csv(csv_url)\n",
    "\n",
    "print(\"--- DataFrame from CSV (head) ---\")\n",
    "print(df_csv.head())\n",
    "\n",
    "print(\"\\n--- DataFrame Info ---\")\n",
    "df_csv.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to CSV\n",
    "\n",
    "We'll save a portion of our `df_csv` to a new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a subset of the DataFrame for writing\n",
    "df_csv_subset = df_csv.head(100)\n",
    "\n",
    "# Define the output file path\n",
    "output_csv_path = 'housing_subset.csv'\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "# index=False prevents writing the DataFrame index as a column\n",
    "df_csv_subset.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"DataFrame saved to {output_csv_path}\")\n",
    "\n",
    "# Verify by reading the new CSV file\n",
    "df_verify_csv = pd.read_csv(output_csv_path)\n",
    "print(\"\\n--- Verified CSV (head) ---\")\n",
    "print(df_verify_csv.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Excel Files (.xlsx, .xls)\n",
    "\n",
    "Excel files can contain multiple sheets and more complex formatting. Pandas simplifies reading data from specific sheets.\n",
    "\n",
    "**Note for Excel Data:** The dataset provided (`https://archive.ics.uci.edu/dataset/352/online+retail`) is a `.zip` file. You need to download it manually, extract it, and place `Online Retail.xlsx` in the same directory as your Jupyter notebook, or provide its full path. Due to the nature of web servers, it's generally not possible to directly read `.xlsx` files from arbitrary web URLs without specific server configurations or using a library that can extract from zip streams.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Download the file from: `https://archive.ics.uci.edu/static/public/352/online+retail.zip`\n",
    "2.  Extract the `Online Retail.xlsx` file.\n",
    "3.  Place `Online Retail.xlsx` in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local file path for the extracted Excel file\n",
    "excel_file_path = 'Online Retail.xlsx'\n",
    "\n",
    "if os.path.exists(excel_file_path):\n",
    "    # Read the first 100 rows from the first sheet (default sheet_name=0)\n",
    "    df_excel = pd.read_excel(excel_file_path, nrows=100)\n",
    "    \n",
    "    print(\"--- DataFrame from Excel (head) ---\")\n",
    "    print(df_excel.head())\n",
    "    \n",
    "    print(\"\\n--- DataFrame Info ---\")\n",
    "    df_excel.info()\n",
    "else:\n",
    "    print(f\"Error: {excel_file_path} not found. Please download and extract the Excel file as instructed above.\")\n",
    "    df_excel = pd.DataFrame() # Create an empty DataFrame to avoid errors later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to Excel\n",
    "\n",
    "We'll save a processed version of `df_excel` to a new Excel file, potentially on a specific sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_excel.empty:\n",
    "    # Add a dummy 'TotalCost' column for demonstration\n",
    "    df_excel['TotalCost'] = df_excel['Quantity'] * df_excel['UnitPrice']\n",
    "    \n",
    "    # Define the output file path\n",
    "    output_excel_path = 'processed_online_retail.xlsx'\n",
    "    \n",
    "    # Write the DataFrame to an Excel file on a specific sheet\n",
    "    df_excel.to_excel(output_excel_path, index=False, sheet_name='ProcessedData')\n",
    "    \n",
    "    print(f\"DataFrame saved to {output_excel_path} on sheet 'ProcessedData'\")\n",
    "    \n",
    "    # Verify by reading the new Excel file\n",
    "    df_verify_excel = pd.read_excel(output_excel_path, sheet_name='ProcessedData')\n",
    "    print(\"\\n--- Verified Excel (head) ---\")\n",
    "    print(df_verify_excel.head())\n",
    "else:\n",
    "    print(\"Skipping Excel write operation as df_excel is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SQLite Databases\n",
    "\n",
    "SQLite is a lightweight, file-based SQL database. Python's `sqlite3` module provides native support, and Pandas integrates seamlessly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection Setup\n",
    "\n",
    "We'll use an in-memory database (`:memory:`) for simplicity, but you can replace this with a file path like `'my_data.db'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to an in-memory SQLite database\n",
    "conn = sqlite3.connect(':memory:')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create a dummy table and insert some data\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS users (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    name TEXT NOT NULL,\n",
    "    email TEXT UNIQUE\n",
    ");\n",
    "''')\n",
    "\n",
    "sample_users = [\n",
    "    (1, 'Alice', 'alice@example.com'),\n",
    "    (2, 'Bob', 'bob@example.com'),\n",
    "    (3, 'Charlie', 'charlie@example.com')\n",
    "]\n",
    "cursor.executemany(\"INSERT OR IGNORE INTO users (id, name, email) VALUES (?, ?, ?);\", sample_users)\n",
    "conn.commit()\n",
    "\n",
    "print(\"SQLite database setup complete (in-memory).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from SQLite\n",
    "\n",
    "Use `pd.read_sql_query()` to execute SQL and get a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all data from the 'users' table\n",
    "df_sql_read = pd.read_sql_query(\"SELECT * FROM users;\", conn)\n",
    "\n",
    "print(\"--- DataFrame from SQLite (head) ---\")\n",
    "print(df_sql_read.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing (Inserting) to SQLite\n",
    "\n",
    "Use `df.to_sql()` to insert DataFrame contents into a table. Note the `if_exists` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with new user data\n",
    "new_users_df = pd.DataFrame({\n",
    "    'id': [4, 5],\n",
    "    'name': ['Diana', 'Eve'],\n",
    "    'email': ['diana@example.com', 'eve@example.com']\n",
    "})\n",
    "\n",
    "# Append new users to the 'users' table\n",
    "new_users_df.to_sql('users', conn, if_exists='append', index=False)\n",
    "\n",
    "print(\"New users added to SQLite via df.to_sql().\")\n",
    "\n",
    "# Verify insertion\n",
    "print(\"\\n--- Users after insertion ---\")\n",
    "print(pd.read_sql_query(\"SELECT * FROM users;\", conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Data in SQLite (Direct SQL)\n",
    "\n",
    "While `to_sql(if_exists='replace')` can overwrite entire tables, for targeted updates, direct SQL `UPDATE` statements are more efficient and safer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Bob's email using a direct SQL UPDATE statement\n",
    "cursor.execute(\"UPDATE users SET email = ? WHERE name = ?;\", ('bob.new@example.com', 'Bob'))\n",
    "conn.commit()\n",
    "\n",
    "print(\"Bob's email updated in SQLite.\")\n",
    "\n",
    "# Verify update\n",
    "print(\"\\n--- Users after update ---\")\n",
    "print(pd.read_sql_query(\"SELECT * FROM users;\", conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting Data from SQLite (Direct SQL)\n",
    "\n",
    "Similar to updates, direct SQL `DELETE` statements are used for removing specific records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete user 'Charlie' using a direct SQL DELETE statement\n",
    "cursor.execute(\"DELETE FROM users WHERE name = ?;\", ('Charlie',))\n",
    "conn.commit()\n",
    "\n",
    "print(\"Charlie deleted from SQLite.\")\n",
    "\n",
    "# Verify deletion\n",
    "print(\"\\n--- Users after deletion ---\")\n",
    "print(pd.read_sql_query(\"SELECT * FROM users;\", conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close SQLite Connection\n",
    "\n",
    "Always close your database connection when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()\n",
    "print(\"SQLite connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. APIs (Application Programming Interfaces)\n",
    "\n",
    "APIs are interfaces for web services to exchange data. We use the `requests` library to make HTTP calls and then convert JSON responses into DataFrames. Pandas does *not* have a direct `to_api()` method; instead, you build payloads and use `requests` for writing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from APIs (GET Requests)\n",
    "\n",
    "We'll use `jsonplaceholder.typicode.com` which provides dummy REST API data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_get_url = \"https://jsonplaceholder.typicode.com/todos\" # Endpoint for todos\n",
    "\n",
    "try:\n",
    "    response = requests.get(api_get_url)\n",
    "    response.raise_for_status() # Raises an HTTPError for bad responses (4xx or 5xx)\n",
    "    \n",
    "    data = response.json() # Parse the JSON response into a Python list/dict\n",
    "    df_api_read = pd.DataFrame(data)\n",
    "    \n",
    "    print(\"--- DataFrame from API (head) ---\")\n",
    "    print(df_api_read.head())\n",
    "\n",
    "    print(\"\\n--- DataFrame Info ---\")\n",
    "    df_api_read.info()\n",
    "    \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching data from API: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing (Sending) Data to APIs (POST/PUT Requests)\n",
    "\n",
    "For writing to APIs, you construct the data payload (often as a Python dictionary, converted to JSON) and send it using `requests.post()` (for creating new resources) or `requests.put()`/`requests.patch()` (for updating existing resources). Pandas itself doesn't directly manage this, but it can help prepare the data to be sent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json # Needed to convert Python dict to JSON string\n",
    "\n",
    "api_post_url = \"https://jsonplaceholder.typicode.com/posts\" # Endpoint for creating posts\n",
    "\n",
    "# Example: Create a new post from a dictionary\n",
    "new_post_payload = {\n",
    "    'title': 'My New Blog Post',\n",
    "    'body': 'This is the content of my exciting new blog post.',\n",
    "    'userId': 101 # A fictional user ID\n",
    "}\n",
    "\n",
    "# Convert the Python dictionary to a JSON string\n",
    "json_data_to_send = json.dumps(new_post_payload)\n",
    "\n",
    "# Set the Content-Type header to indicate we are sending JSON\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "print(\"Attempting to send new post to API...\")\n",
    "\n",
    "try:\n",
    "    response_post = requests.post(api_post_url, data=json_data_to_send, headers=headers)\n",
    "    response_post.raise_for_status() # Check for HTTP errors\n",
    "    \n",
    "    print(\"New post created successfully via POST!\")\n",
    "    print(\"API Response:\", response_post.json()) # The API typically returns the created resource with its new ID\n",
    "    \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error sending data to API: {e}\")\n",
    "\n",
    "\n",
    "# --- Example for PUT (Updating an existing resource) ---\n",
    "api_put_url = \"https://jsonplaceholder.typicode.com/posts/1\" # Updating post with ID 1\n",
    "\n",
    "updated_post_payload = {\n",
    "    'id': 1, # Often, the ID is included in the payload for PUT requests\n",
    "    'title': 'Updated Title for Post 1',\n",
    "    'body': 'This content has been revised and updated.',\n",
    "    'userId': 1\n",
    "}\n",
    "json_data_to_update = json.dumps(updated_post_payload)\n",
    "\n",
    "print(\"\\nAttempting to update post ID 1 via PUT...\")\n",
    "\n",
    "try:\n",
    "    response_put = requests.put(api_put_url, data=json_data_to_update, headers=headers)\n",
    "    response_put.raise_for_status()\n",
    "    \n",
    "    print(\"Post ID 1 updated successfully via PUT!\")\n",
    "    print(\"API Response:\", response_put.json())\n",
    "    \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error updating data via API: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook covers the essential methods for data ingress (reading) and egress (writing/updating/deleting) with Pandas for common data formats and sources. Mastering these operations is foundational for any data analysis or engineering task in Python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

