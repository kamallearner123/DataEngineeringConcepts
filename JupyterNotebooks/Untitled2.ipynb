{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f03a6b9-a722-4a0c-8789-2361a055ceba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error connecting to Redshift Serverless: connect() got an unexpected keyword argument 'workgroup_name'\n",
      "Ensure your AWS credentials are configured (e.g., via aws configure),\n",
      "your security group allows access, and IAM permissions are correct.\n"
     ]
    }
   ],
   "source": [
    "import redshift_connector\n",
    "\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import boto3 # For S3 operations\n",
    "\n",
    "# --- Redshift Serverless and S3 Configuration (replace with your actual details) ---\n",
    "AWS_REGION = 'ap-south-1' # Your AWS region\n",
    "REDSHIFT_SERVERLESS_WORKGROUP_NAME = 'default-workgroup'\n",
    "REDSHIFT_SERVERLESS_DB_NAME = 'dev' # Or your specific database name\n",
    "# If using IAM Role for the executing environment (e.g., EC2, Lambda),\n",
    "# you typically don't need access_key_id/secret_access_key here.\n",
    "# redshift_connector will pick up credentials from env vars, ~/.aws/credentials, or IAM role.\n",
    "# For local testing without an IAM role:\n",
    "# AWS_ACCESS_KEY_ID = 'YOUR_AWS_ACCESS_KEY_ID'\n",
    "# AWS_SECRET_ACCESS_KEY = 'YOUR_AWS_SECRET_ACCESS_KEY'\n",
    "\n",
    "S3_KEY_PREFIX = 'serverless-data-uploads/'\n",
    "# S3 bucket for COPY/UNLOAD (must exist and have Redshift role access)\n",
    "S3_BUCKET_NAME = 'kkm2-unique-test-bucket-2025-06-26-py-1'\n",
    "S3_FILE_TO_UNLOAD = 'redshift_sample.csv'\n",
    "S3_UNLOAD_PATH = f's3://{S3_BUCKET_NAME}/{S3_FILE_TO_UNLOAD}'\n",
    "S3_COPY_PATH = f's3://{S3_BUCKET_NAME}/flights.csv' # Path to your existing sample data\n",
    "\n",
    "# IAM Role ARN for Redshift to access S3 (must have s3:GetObject on your bucket)\n",
    "# This is attached to your Redshift Serverless namespace.\n",
    "IAM_ROLE_ARN_FOR_COPY = 'arn:aws:iam::123456789012:role/YourRedshiftServerlessCopyRole'\n",
    "\n",
    "def get_redshift_serverless_connection():\n",
    "    \"\"\"Establishes a connection to Amazon Redshift Serverless using IAM authentication.\"\"\"\n",
    "    try:\n",
    "        conn = redshift_connector.connect(\n",
    "            workgroup_name=REDSHIFT_SERVERLESS_WORKGROUP_NAME,\n",
    "            database=REDSHIFT_SERVERLESS_DB_NAME,\n",
    "            iam=True,\n",
    "            region=AWS_REGION\n",
    "            # ... other parameters\n",
    "        )\n",
    "        print(\"Successfully connected to Redshift Serverless!\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Redshift Serverless: {e}\")\n",
    "        print(\"Ensure your AWS credentials are configured (e.g., via aws configure),\")\n",
    "        print(\"your security group allows access, and IAM permissions are correct.\")\n",
    "        return None\n",
    "\n",
    "def list_redshift_databases(conn):\n",
    "    \"\"\"Lists all databases in the Redshift Serverless workgroup.\"\"\"\n",
    "    if not conn:\n",
    "        print(\"No active connection to Redshift.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            # For Redshift Serverless, querying pg_database will show available databases within the namespace\n",
    "            cursor.execute(\"SELECT datname FROM pg_database;\")\n",
    "            databases = [row[0] for row in cursor.fetchall()]\n",
    "            print(\"\\nRedshift Serverless Databases:\")\n",
    "            for db in databases:\n",
    "                print(f\"- {db}\")\n",
    "            return databases\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing databases: {e}\")\n",
    "        return []\n",
    "get_redshift_serverless_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "673fe49a-6fc2-4d37-9121-29b4ed452296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: redshift_connector[full] in /Users/kamalmukiri/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages (2.1.7)\n",
      "Collecting redshift_connector[full]\n",
      "  Downloading redshift_connector-2.1.8-py3-none-any.whl.metadata (69 kB)\n",
      "Requirement already satisfied: scramp<1.5.0,>=1.2.0 in /Users/kamalmukiri/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages (from redshift_connector[full]) (1.4.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kamalmukiri/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages (from redshift_connector[full]) (2025.1)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.7.0 in /Users/kamalmukiri/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages (from redshift_connector[full]) (4.13.3)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.9.201 in /Users/kamalmukiri/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages (from redshift_connector[full]) (1.38.44)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /Users/kamalmukiri/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages (from redshift_connector[full]) (2.32.3)\n",
      "Requirement already satisfied: lxml<6.0.0,>=4.6.5 in /Users/kamalmukiri/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages (from redshift_connector[full]) (5.3.1)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.12.201 in /Users/kamalmukiri/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages (from redshift_connector[full]) (1.38.44)\n",
      "Requirement already satisfied: packaging in /Users/kamalmukiri/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages (from redshift_connector[full]) (24.2)\n",
      "Requirement already satisfied: setuptools in /Users/kamalmukiri/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages (from redshift_connector[full]) (76.0.0)\n",
      "Requirement already satisfied: numpy in /Users/kamalmukiri/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages (from redshift_connector[full]) (2.2.3)\n",
      "Requirement already satisfied: pandas in /Users/kamalmukiri/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages (from redshift_connector[full]) (2.2.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/kamalmukiri/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages (from beautifulsoup4<5.0.0,>=4.7.0->redshift_connector[full]) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/kamalmukiri/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages (from beautifulsoup4<5.0.0,>=4.7.0->redshift_connector[full]) (4.12.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/kamalmukiri/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages (from boto3<2.0.0,>=1.9.201->redshift_connector[full]) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /Users/kamalmukiri/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages (from boto3<2.0.0,>=1.9.201->redshift_connector[full]) (0.13.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/kamalmukiri/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages (from botocore<2.0.0,>=1.12.201->redshift_connector[full]) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /Users/kamalmukiri/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages (from botocore<2.0.0,>=1.12.201->redshift_connector[full]) (2.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kamalmukiri/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.12.201->redshift_connector[full]) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kamalmukiri/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages (from requests<3.0.0,>=2.23.0->redshift_connector[full]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kamalmukiri/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages (from requests<3.0.0,>=2.23.0->redshift_connector[full]) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kamalmukiri/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages (from requests<3.0.0,>=2.23.0->redshift_connector[full]) (2025.1.31)\n",
      "Requirement already satisfied: asn1crypto>=1.5.1 in /Users/kamalmukiri/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages (from scramp<1.5.0,>=1.2.0->redshift_connector[full]) (1.5.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/kamalmukiri/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages (from pandas->redshift_connector[full]) (2025.1)\n",
      "Downloading redshift_connector-2.1.8-py3-none-any.whl (139 kB)\n",
      "Installing collected packages: redshift_connector\n",
      "  Attempting uninstall: redshift_connector\n",
      "    Found existing installation: redshift-connector 2.1.7\n",
      "    Uninstalling redshift-connector-2.1.7:\n",
      "      Successfully uninstalled redshift-connector-2.1.7\n",
      "Successfully installed redshift_connector-2.1.8\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade \"redshift_connector[full]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e59c72fb-937a-4ded-a0e3-7580c0fa5e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error getting Redshift Serverless endpoint or credentials: An error occurred (ResourceNotFoundException) when calling the GetWorkgroup operation: Serverless workgroup demo-workgroup not found.\n",
      "Ensure your AWS credentials are configured and have `redshift-serverless:GetWorkgroup`\n",
      "and `redshift-data:GetCredentials` permissions.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "# --- Redshift Serverless and S3 Configuration (replace with your actual details) ---\n",
    "AWS_REGION = 'ap-south-1' # Your AWS region\n",
    "REDSHIFT_SERVERLESS_WORKGROUP_NAME = 'demo-workgroup'\n",
    "REDSHIFT_SERVERLESS_DB_NAME = 'dev' # Or your specific database name\n",
    "# This user needs to exist in your Redshift Serverless database and be mapped to an IAM role/user\n",
    "# that has GetCredentials permission.\n",
    "REDSHIFT_DB_USER = 'admin'\n",
    "\n",
    "S3_BUCKET_NAME = 'your-unique-serverless-redshift-s3-bucket'\n",
    "S3_KEY_PREFIX = 'serverless-data-uploads/'\n",
    "# IAM Role ARN for Redshift Serverless to access S3 for COPY commands.\n",
    "# This role must be associated with your Redshift Serverless namespace.\n",
    "IAM_ROLE_ARN_FOR_COPY = 'arn:aws:iam::310879042055:role/aws-service-role/redshift.amazonaws.com/AWSServiceRoleForRedshift'\n",
    "\n",
    "\n",
    "def get_redshift_serverless_endpoint_and_credentials():\n",
    "    \"\"\"\n",
    "    Obtains the Redshift Serverless workgroup endpoint and temporary database credentials\n",
    "    using boto3.\n",
    "    \"\"\"\n",
    "    redshift_serverless_client = boto3.client('redshift-serverless', region_name=AWS_REGION)\n",
    "    redshift_data_client = boto3.client('redshift-data', region_name=AWS_REGION)\n",
    "\n",
    "    try:\n",
    "        # 1. Get Workgroup details to find the endpoint\n",
    "        workgroup_response = redshift_serverless_client.get_workgroup(\n",
    "            workgroupName=REDSHIFT_SERVERLESS_WORKGROUP_NAME\n",
    "        )\n",
    "        endpoint_address = workgroup_response['workgroup']['endpoint']['address']\n",
    "        endpoint_port = workgroup_response['workgroup']['endpoint']['port']\n",
    "        print(f\"Redshift Serverless Endpoint: {endpoint_address}:{endpoint_port}\")\n",
    "\n",
    "        # 2. Get temporary database credentials\n",
    "        # Note: GetClusterCredentials for provisioned, GetCredentials for serverless\n",
    "        credentials_response = redshift_data_client.get_credentials(\n",
    "            workgroupName=REDSHIFT_SERVERLESS_WORKGROUP_NAME,\n",
    "            dbName=REDSHIFT_SERVERLESS_DB_NAME,\n",
    "            dbUser=REDSHIFT_DB_USER,\n",
    "            # autoCreate is useful if the user doesn't exist, but needs permission for it.\n",
    "            # autoCreate=False\n",
    "        )\n",
    "\n",
    "        db_user = credentials_response['DbUser']\n",
    "        db_password = credentials_response['DbPassword']\n",
    "        temp_token = credentials_response['TempUserPassphrase'] # This is the password\n",
    "\n",
    "        return {\n",
    "            'host': endpoint_address,\n",
    "            'port': endpoint_port,\n",
    "            'user': db_user,\n",
    "            'password': temp_token,\n",
    "            'dbname': REDSHIFT_SERVERLESS_DB_NAME\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting Redshift Serverless endpoint or credentials: {e}\")\n",
    "        print(\"Ensure your AWS credentials are configured and have `redshift-serverless:GetWorkgroup`\")\n",
    "        print(\"and `redshift-data:GetCredentials` permissions.\")\n",
    "        return None\n",
    "\n",
    "def get_psycopg2_redshift_serverless_connection():\n",
    "    \"\"\"Establishes a connection to Redshift Serverless using psycopg2 with temporary credentials.\"\"\"\n",
    "    conn_params = get_redshift_serverless_endpoint_and_credentials()\n",
    "    if not conn_params:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        conn = psycopg2.connect(**conn_params)\n",
    "        print(\"Successfully connected to Redshift Serverless using psycopg2!\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Redshift Serverless with psycopg2: {e}\")\n",
    "        print(\"Ensure security group allows access and network path is open.\")\n",
    "        return None\n",
    "\n",
    "def list_redshift_databases_psycopg2(conn):\n",
    "    \"\"\"Lists all databases in the Redshift Serverless workgroup using psycopg2.\"\"\"\n",
    "    if not conn:\n",
    "        print(\"No active connection to Redshift.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(\"SELECT datname FROM pg_database;\")\n",
    "            databases = [row[0] for row in cursor.fetchall()]\n",
    "            print(\"\\nRedshift Serverless Databases (via psycopg2):\")\n",
    "            for db in databases:\n",
    "                print(f\"- {db}\")\n",
    "            return databases\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing databases with psycopg2: {e}\")\n",
    "        return []\n",
    "\n",
    "def upload_data_to_redshift_serverless_via_s3_copy_psycopg2(\n",
    "    conn,\n",
    "    df,\n",
    "    table_name,\n",
    "    s3_bucket,\n",
    "    s3_key_prefix,\n",
    "    iam_role_arn_for_copy,\n",
    "    create_table_sql=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Uploads a pandas DataFrame to Redshift Serverless using S3 and the COPY command via psycopg2.\n",
    "    \"\"\"\n",
    "    if not conn:\n",
    "        print(\"No active connection to Redshift Serverless.\")\n",
    "        return\n",
    "\n",
    "    s3_client = boto3.client('s3', region_name=AWS_REGION)\n",
    "\n",
    "    csv_buffer = StringIO()\n",
    "    df.to_csv(csv_buffer, index=False, header=False, sep=',') # Ensure no header or index\n",
    "\n",
    "    s3_file_name = f\"{s3_key_prefix}{table_name}_{uuid.uuid4()}.csv\" # Use uuid for uniqueness\n",
    "    s3_path = f\"s3://{s3_bucket}/{s3_file_name}\"\n",
    "\n",
    "    try:\n",
    "        # 1. Upload DataFrame to S3\n",
    "        s3_client.put_object(Bucket=s3_bucket, Key=s3_file_name, Body=csv_buffer.getvalue())\n",
    "        print(f\"Data uploaded to S3: {s3_path}\")\n",
    "\n",
    "        with conn.cursor() as cursor:\n",
    "            # Optional: Create table if it doesn't exist\n",
    "            if create_table_sql:\n",
    "                try:\n",
    "                    cursor.execute(create_table_sql)\n",
    "                    conn.commit()\n",
    "                    print(f\"Table '{table_name}' created (if it didn't exist).\")\n",
    "                except psycopg2.ProgrammingError as e:\n",
    "                    if \"already exists\" in str(e):\n",
    "                        print(f\"Table '{table_name}' already exists.\")\n",
    "                    else:\n",
    "                        raise e # Re-raise other errors\n",
    "\n",
    "            # 2. Use Redshift COPY command to load data from S3\n",
    "            copy_command = f\"\"\"\n",
    "            COPY {table_name}\n",
    "            FROM '{s3_path}'\n",
    "            IAM_ROLE '{iam_role_arn_for_copy}'\n",
    "            CSV\n",
    "            IGNOREHEADER 0;\n",
    "            \"\"\"\n",
    "            print(f\"Executing COPY command for table '{table_name}'...\")\n",
    "            cursor.execute(copy_command)\n",
    "            conn.commit()\n",
    "            print(f\"Data successfully loaded into Redshift Serverless table '{table_name}' from S3.\")\n",
    "\n",
    "            # Optional: Clean up S3 file after successful load\n",
    "            s3_client.delete_object(Bucket=s3_bucket, Key=s3_file_name)\n",
    "            print(f\"Temporary S3 file deleted: {s3_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        conn.rollback() # Rollback on error\n",
    "        print(f\"Error during data upload with psycopg2: {e}\")\n",
    "    finally:\n",
    "        csv_buffer.close()\n",
    "\n",
    "\n",
    "# Example Usage with psycopg2\n",
    "if __name__ == \"__main__\":\n",
    "    conn = get_psycopg2_redshift_serverless_connection()\n",
    "\n",
    "    if conn:\n",
    "        # List databases\n",
    "        list_redshift_databases_psycopg2(conn)\n",
    "\n",
    "        # --- Example Data for Upload ---\n",
    "        data = {\n",
    "            'item_id': [201, 202, 203],\n",
    "            'item_name': ['Book', 'Pen', 'Notebook'],\n",
    "            'category': ['Stationery', 'Stationery', 'Stationery'],\n",
    "            'price_usd': [25.00, 2.50, 8.00]\n",
    "        }\n",
    "        df_to_upload = pd.DataFrame(data)\n",
    "        target_table = 'serverless_inventory'\n",
    "\n",
    "        # SQL to create the target table (adjust columns and types as needed)\n",
    "        create_table_sql_statement = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {target_table} (\n",
    "            item_id INT,\n",
    "            item_name VARCHAR(255),\n",
    "            category VARCHAR(255),\n",
    "            price_usd DECIMAL(10, 2)\n",
    "        );\n",
    "        \"\"\"\n",
    "\n",
    "        upload_data_to_redshift_serverless_via_s3_copy_psycopg2(\n",
    "            conn,\n",
    "            df_to_upload,\n",
    "            target_table,\n",
    "            S3_BUCKET_NAME,\n",
    "            S3_KEY_PREFIX,\n",
    "            IAM_ROLE_ARN_FOR_COPY,\n",
    "            create_table_sql=create_table_sql_statement\n",
    "        )\n",
    "\n",
    "        # Close the connection\n",
    "        conn.close()\n",
    "        print(\"Redshift Serverless (psycopg2) connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a4a8d4-50bc-4bef-8509-e9c1a4d2f136",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
