{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a2bf95d-a5c9-4c0e-bac5-6fbdeaca4ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Redshift Python Demo ---\n",
      "\n",
      "1. Establishing secure connection to Redshift using IAM...\n",
      "Error connecting to Redshift: name 'REDSHIFT_SERVERLESS_WORKGROUP_NAME' is not defined\n",
      "Please ensure your AWS CLI is configured, Redshift cluster is accessible,\n",
      "and the IAM user/role has 'redshift:GetClusterCredentials' permission.\n",
      "\n",
      "2. Executing a simple SELECT query:\n",
      "Error executing query: name 'cursor' is not defined\n",
      "\n",
      "3. Demonstrating a parameterized query:\n",
      "Error with parameterized query: name 'cursor' is not defined\n",
      "\n",
      "4. Inserting new data:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'conn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 99\u001b[39m\n\u001b[32m     98\u001b[39m new_user_city = \u001b[33m'\u001b[39m\u001b[33mSeattle\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m \u001b[43mcursor\u001b[49m.execute(\u001b[33m\"\u001b[39m\u001b[33mINSERT INTO users (id, name, city) VALUES (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m);\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    100\u001b[39m                (new_user_id, new_user_name, new_user_city))\n\u001b[32m    101\u001b[39m conn.commit() \u001b[38;5;66;03m# Commit the transaction\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'cursor' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 110\u001b[39m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28mprint\u001b[39m(cursor.fetchone())\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     \u001b[43mconn\u001b[49m.rollback() \u001b[38;5;66;03m# Rollback on error\u001b[39;00m\n\u001b[32m    111\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError inserting data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# --- 5. UNLOAD Data to S3 ---\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# Requires IAM role associated with Redshift cluster to have S3 write permissions\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'conn' is not defined"
     ]
    }
   ],
   "source": [
    "import redshift_connector\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd # Optional, for showing dataframes\n",
    "\n",
    "# --- Configuration ---\n",
    "# Replace with your actual Redshift cluster details\n",
    "DATABASE_NAME='dev' # Or your specific database name\n",
    "DB_USER='admin' # A Redshift database user that exists in your cluster\n",
    "AWS_REGION='us-east-1' # e.g., 'us-east-1'\n",
    "REDSHIFT_SERVERLESS_WORKGROUP_NAME='demo-workgroup-01'\n",
    "\n",
    "# S3 bucket for COPY/UNLOAD (must exist and have Redshift role access)\n",
    "S3_BUCKET_NAME = 'kkm2-unique-test-bucket-2025-06-26-py-1'\n",
    "S3_FILE_TO_UNLOAD = 'redshift_sample.csv'\n",
    "S3_UNLOAD_PATH = f's3://{S3_BUCKET_NAME}/{S3_FILE_TO_UNLOAD}'\n",
    "S3_COPY_PATH = f's3://{S3_BUCKET_NAME}/flights.csv' # Path to your existing sample data\n",
    "\n",
    "\n",
    "\n",
    "# Endpoint\n",
    "# default-workgroup.310879042055.us-east-1.redshift-serverless.amazonaws.com:5439/dev\n",
    "# JDBC URL\n",
    "# jdbc:redshift://default-workgroup.310879042055.us-east-1.redshift-serverless.amazonaws.com:5439/dev\n",
    "# ODBC URL\n",
    "# Driver={Amazon Redshift (x64)}; Server=default-workgroup.310879042055.us-east-1.redshift-serverless.amazonaws.com; Database=dev\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ensure your AWS CLI is configured or environment variables are set\n",
    "# AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_DEFAULT_REGION\n",
    "\n",
    "print(\"--- Starting Redshift Python Demo ---\")\n",
    "\n",
    "# --- 1. Securely Connect using IAM Authentication ---\n",
    "# This is the recommended way. redshift_connector uses boto3's underlying\n",
    "# credential chain (environment variables, ~/.aws/credentials, IAM roles).\n",
    "print(\"\\n1. Establishing secure connection to Redshift using IAM...\")\n",
    "try:\n",
    "    conn = redshift_connector.connect(\n",
    "        iam=True,\n",
    "        # For SERVERLESS, use workgroup_name\n",
    "        workgroup_name=REDSHIFT_SERVERLESS_WORKGROUP_NAME, # <<<<<< Make sure this line is present\n",
    "        database=DATABASE_NAME,\n",
    "        db_user=DB_USER,\n",
    "        region=AWS_REGION\n",
    "    )\n",
    "\n",
    "    # conn = redshift_connector.connect(\n",
    "    #     iam=True, # Enable IAM authentication\n",
    "    #     # workgroup_name=WORKGROUP_NAME, # Specify the Redshift Serverless Workgroup\n",
    "    #     database=DATABASE_NAME,\n",
    "    #     # db_user=DB_USER, # Often not needed if using IAM authentication\n",
    "    #     region=AWS_REGION\n",
    "    # )\n",
    "    print(\"REDSHIFT: Connection successful!\")\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Redshift: {e}\")\n",
    "    print(\"Please ensure your AWS CLI is configured, Redshift cluster is accessible,\")\n",
    "    print(\"and the IAM user/role has 'redshift:GetClusterCredentials' permission.\")\n",
    "    exit() # Exit if connection fails, as nothing else will work\n",
    "\n",
    "# --- 2. Execute a Simple Query ---\n",
    "print(\"\\n2. Executing a simple SELECT query:\")\n",
    "try:\n",
    "    cursor.execute(\"SELECT id, name, city FROM users LIMIT 5;\")\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    if results:\n",
    "        print(\"Query Results:\")\n",
    "        for row in results:\n",
    "            print(row)\n",
    "    else:\n",
    "        print(\"No results found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error executing query: {e}\")\n",
    "\n",
    "# --- 3. Parameterized Query (Prevent SQL Injection) ---\n",
    "print(\"\\n3. Demonstrating a parameterized query:\")\n",
    "try:\n",
    "    city_filter = 'London'\n",
    "    cursor.execute(\"SELECT id, name FROM users WHERE city = %s;\", (city_filter,))\n",
    "    filtered_results = cursor.fetchall()\n",
    "    print(f\"Users in {city_filter}:\")\n",
    "    for row in filtered_results:\n",
    "        print(row)\n",
    "except Exception as e:\n",
    "    print(f\"Error with parameterized query: {e}\")\n",
    "\n",
    "# --- 4. INSERT Data ---\n",
    "print(\"\\n4. Inserting new data:\")\n",
    "try:\n",
    "    new_user_id = 4\n",
    "    new_user_name = 'Alice Wonderland'\n",
    "    new_user_city = 'Seattle'\n",
    "    cursor.execute(\"INSERT INTO users (id, name, city) VALUES (%s, %s, %s);\",\n",
    "                   (new_user_id, new_user_name, new_user_city))\n",
    "    conn.commit() # Commit the transaction\n",
    "    print(f\"Inserted user: {new_user_name}\")\n",
    "\n",
    "    # Verify insertion\n",
    "    cursor.execute(f\"SELECT * FROM users WHERE id = {new_user_id};\")\n",
    "    print(\"Verifying insertion:\")\n",
    "    print(cursor.fetchone())\n",
    "\n",
    "except Exception as e:\n",
    "    conn.rollback() # Rollback on error\n",
    "    print(f\"Error inserting data: {e}\")\n",
    "\n",
    "# --- 5. UNLOAD Data to S3 ---\n",
    "# Requires IAM role associated with Redshift cluster to have S3 write permissions\n",
    "print(\"\\n5. Unloading data to S3:\")\n",
    "try:\n",
    "    # Ensure the IAM role attached to your Redshift cluster has write access to S3_BUCKET_NAME\n",
    "    # Example: 'arn:aws:iam::ACCOUNT_ID:role/YourRedshiftClusterIAMRole'\n",
    "    iam_role_arn = 'arn:aws:iam::YOUR_AWS_ACCOUNT_ID:role/YourRedshiftClusterIAMRole' # REPLACE THIS\n",
    "\n",
    "    unload_query = f\"\"\"\n",
    "    UNLOAD ('SELECT id, name, city FROM users')\n",
    "    TO '{S3_UNLOAD_PATH}'\n",
    "    IAM_ROLE '{iam_role_arn}'\n",
    "    CSV\n",
    "    HEADER\n",
    "    PARALLEL OFF; -- For small demos, PARALLEL OFF makes it one file. Remove for large datasets.\n",
    "    \"\"\"\n",
    "    print(f\"Executing UNLOAD query to {S3_UNLOAD_PATH}...\")\n",
    "    cursor.execute(unload_query)\n",
    "    conn.commit()\n",
    "    print(\"Data successfully unloaded to S3. Check your S3 bucket!\")\n",
    "    # Optional: Verify S3 file exists using boto3\n",
    "    s3_client = boto3.client('s3', region_name=AWS_REGION)\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=S3_BUCKET_NAME, Key=S3_FILE_TO_UNLOAD + '000') # Check for part file\n",
    "        print(f\"Confirmed file {S3_FILE_TO_UNLOAD} exists in S3.\")\n",
    "    except Exception as s3_e:\n",
    "        print(f\"Could not confirm S3 file presence (may take a moment to appear): {s3_e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    conn.rollback()\n",
    "    print(f\"Error unloading data to S3: {e}\")\n",
    "    print(\"Ensure the IAM role associated with your Redshift cluster has S3 write permissions.\")\n",
    "\n",
    "# --- 6. (Optional) Show Data API usage with boto3 ---\n",
    "# This is an alternative to persistent connections, good for serverless/asynchronous needs.\n",
    "print(\"\\n6. (Optional) Demonstrating Redshift Data API (using boto3):\")\n",
    "try:\n",
    "    redshift_data_client = boto3.client('redshift-data', region_name=AWS_REGION)\n",
    "\n",
    "    # Execute a statement\n",
    "    statement_response = redshift_data_client.execute_statement(\n",
    "        ClusterIdentifier=CLUSTER_IDENTIFIER,\n",
    "        Database=DATABASE_NAME,\n",
    "        DbUser=DB_USER, # The Redshift database user\n",
    "        Sql=\"SELECT * FROM users WHERE id = 1;\"\n",
    "    )\n",
    "    statement_id = statement_response['Id']\n",
    "    print(f\"Data API Statement ID: {statement_id}\")\n",
    "\n",
    "    # Wait for statement to complete (for demo purposes)\n",
    "    import time\n",
    "    status = ''\n",
    "    while status not in ['FINISHED', 'FAILED', 'ABORTED']:\n",
    "        time.sleep(1)\n",
    "        description_response = redshift_data_client.describe_statement(Id=statement_id)\n",
    "        status = description_response['Status']\n",
    "        print(f\"Statement status: {status}\")\n",
    "\n",
    "    if status == 'FINISHED':\n",
    "        result_response = redshift_data_client.get_statement_result(Id=statement_id)\n",
    "        print(\"Data API Results:\")\n",
    "        # Data API results are in a specific format, you'd usually parse them\n",
    "        if result_response and result_response.get('Records'):\n",
    "            # Extract column names\n",
    "            column_names = [col['label'] for col in result_response['ColumnMetadata']]\n",
    "            print(column_names)\n",
    "            for record in result_response['Records']:\n",
    "                # Each record is a list of dictionaries, where each dict has a key matching the data type (e.g., 'longValue', 'stringValue')\n",
    "                row_values = [list(col.values())[0] for col in record]\n",
    "                print(row_values)\n",
    "        else:\n",
    "            print(\"No results from Data API.\")\n",
    "    else:\n",
    "        print(f\"Data API query failed or was aborted: {description_response.get('Error')}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error using Redshift Data API: {e}\")\n",
    "    print(\"Ensure the IAM user/role has 'redshift-data:*' permissions.\")\n",
    "\n",
    "\n",
    "# --- Cleanup ---\n",
    "print(\"\\n--- Cleaning up ---\")\n",
    "try:\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(\"Connection closed.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during cleanup: {e}\")\n",
    "\n",
    "print(\"\\n--- Demo Complete! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a418fb-f18c-4e00-be27-b1e8876a6dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
