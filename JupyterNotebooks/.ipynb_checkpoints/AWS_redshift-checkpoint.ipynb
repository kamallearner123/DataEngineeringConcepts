{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2bf95d-a5c9-4c0e-bac5-6fbdeaca4ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redshift_connector\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd # Optional, for showing dataframes\n",
    "\n",
    "# --- Configuration ---\n",
    "# Replace with your actual Redshift cluster details\n",
    "CLUSTER_IDENTIFIER = 'your-redshift-cluster-id'\n",
    "DATABASE_NAME = 'dev' # Or your specific database name\n",
    "DB_USER = 'awsuser' # A Redshift database user that exists in your cluster\n",
    "AWS_REGION = 'ap-south-1' # e.g., 'us-east-1'\n",
    "\n",
    "# S3 bucket for COPY/UNLOAD (must exist and have Redshift role access)\n",
    "S3_BUCKET_NAME = 'kkm2-unique-test-bucket-2025-06-26-py-1'\n",
    "S3_FILE_TO_UNLOAD = 'redshift_sample.csv'\n",
    "S3_UNLOAD_PATH = f's3://{S3_BUCKET_NAME}/{S3_FILE_TO_UNLOAD}'\n",
    "S3_COPY_PATH = f's3://{S3_BUCKET_NAME}/flights.csv' # Path to your existing sample data\n",
    "\n",
    "# Ensure your AWS CLI is configured or environment variables are set\n",
    "# AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_DEFAULT_REGION\n",
    "\n",
    "print(\"--- Starting Redshift Python Demo ---\")\n",
    "\n",
    "# --- 1. Securely Connect using IAM Authentication ---\n",
    "# This is the recommended way. redshift_connector uses boto3's underlying\n",
    "# credential chain (environment variables, ~/.aws/credentials, IAM roles).\n",
    "print(\"\\n1. Establishing secure connection to Redshift using IAM...\")\n",
    "try:\n",
    "    conn = redshift_connector.connect(\n",
    "        iam=True, # Enable IAM authentication\n",
    "        cluster_identifier=CLUSTER_IDENTIFIER,\n",
    "        database=DATABASE_NAME,\n",
    "        db_user=DB_USER, # The Redshift database user to authenticate as\n",
    "        region=AWS_REGION\n",
    "    )\n",
    "    print(\"Connection successful!\")\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Redshift: {e}\")\n",
    "    print(\"Please ensure your AWS CLI is configured, Redshift cluster is accessible,\")\n",
    "    print(\"and the IAM user/role has 'redshift:GetClusterCredentials' permission.\")\n",
    "    exit() # Exit if connection fails, as nothing else will work\n",
    "\n",
    "# --- 2. Execute a Simple Query ---\n",
    "print(\"\\n2. Executing a simple SELECT query:\")\n",
    "try:\n",
    "    cursor.execute(\"SELECT id, name, city FROM users LIMIT 5;\")\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    if results:\n",
    "        print(\"Query Results:\")\n",
    "        for row in results:\n",
    "            print(row)\n",
    "    else:\n",
    "        print(\"No results found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error executing query: {e}\")\n",
    "\n",
    "# --- 3. Parameterized Query (Prevent SQL Injection) ---\n",
    "print(\"\\n3. Demonstrating a parameterized query:\")\n",
    "try:\n",
    "    city_filter = 'London'\n",
    "    cursor.execute(\"SELECT id, name FROM users WHERE city = %s;\", (city_filter,))\n",
    "    filtered_results = cursor.fetchall()\n",
    "    print(f\"Users in {city_filter}:\")\n",
    "    for row in filtered_results:\n",
    "        print(row)\n",
    "except Exception as e:\n",
    "    print(f\"Error with parameterized query: {e}\")\n",
    "\n",
    "# --- 4. INSERT Data ---\n",
    "print(\"\\n4. Inserting new data:\")\n",
    "try:\n",
    "    new_user_id = 4\n",
    "    new_user_name = 'Alice Wonderland'\n",
    "    new_user_city = 'Seattle'\n",
    "    cursor.execute(\"INSERT INTO users (id, name, city) VALUES (%s, %s, %s);\",\n",
    "                   (new_user_id, new_user_name, new_user_city))\n",
    "    conn.commit() # Commit the transaction\n",
    "    print(f\"Inserted user: {new_user_name}\")\n",
    "\n",
    "    # Verify insertion\n",
    "    cursor.execute(f\"SELECT * FROM users WHERE id = {new_user_id};\")\n",
    "    print(\"Verifying insertion:\")\n",
    "    print(cursor.fetchone())\n",
    "\n",
    "except Exception as e:\n",
    "    conn.rollback() # Rollback on error\n",
    "    print(f\"Error inserting data: {e}\")\n",
    "\n",
    "# --- 5. UNLOAD Data to S3 ---\n",
    "# Requires IAM role associated with Redshift cluster to have S3 write permissions\n",
    "print(\"\\n5. Unloading data to S3:\")\n",
    "try:\n",
    "    # Ensure the IAM role attached to your Redshift cluster has write access to S3_BUCKET_NAME\n",
    "    # Example: 'arn:aws:iam::ACCOUNT_ID:role/YourRedshiftClusterIAMRole'\n",
    "    iam_role_arn = 'arn:aws:iam::YOUR_AWS_ACCOUNT_ID:role/YourRedshiftClusterIAMRole' # REPLACE THIS\n",
    "\n",
    "    unload_query = f\"\"\"\n",
    "    UNLOAD ('SELECT id, name, city FROM users')\n",
    "    TO '{S3_UNLOAD_PATH}'\n",
    "    IAM_ROLE '{iam_role_arn}'\n",
    "    CSV\n",
    "    HEADER\n",
    "    PARALLEL OFF; -- For small demos, PARALLEL OFF makes it one file. Remove for large datasets.\n",
    "    \"\"\"\n",
    "    print(f\"Executing UNLOAD query to {S3_UNLOAD_PATH}...\")\n",
    "    cursor.execute(unload_query)\n",
    "    conn.commit()\n",
    "    print(\"Data successfully unloaded to S3. Check your S3 bucket!\")\n",
    "    # Optional: Verify S3 file exists using boto3\n",
    "    s3_client = boto3.client('s3', region_name=AWS_REGION)\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=S3_BUCKET_NAME, Key=S3_FILE_TO_UNLOAD + '000') # Check for part file\n",
    "        print(f\"Confirmed file {S3_FILE_TO_UNLOAD} exists in S3.\")\n",
    "    except Exception as s3_e:\n",
    "        print(f\"Could not confirm S3 file presence (may take a moment to appear): {s3_e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    conn.rollback()\n",
    "    print(f\"Error unloading data to S3: {e}\")\n",
    "    print(\"Ensure the IAM role associated with your Redshift cluster has S3 write permissions.\")\n",
    "\n",
    "# --- 6. (Optional) Show Data API usage with boto3 ---\n",
    "# This is an alternative to persistent connections, good for serverless/asynchronous needs.\n",
    "print(\"\\n6. (Optional) Demonstrating Redshift Data API (using boto3):\")\n",
    "try:\n",
    "    redshift_data_client = boto3.client('redshift-data', region_name=AWS_REGION)\n",
    "\n",
    "    # Execute a statement\n",
    "    statement_response = redshift_data_client.execute_statement(\n",
    "        ClusterIdentifier=CLUSTER_IDENTIFIER,\n",
    "        Database=DATABASE_NAME,\n",
    "        DbUser=DB_USER, # The Redshift database user\n",
    "        Sql=\"SELECT * FROM users WHERE id = 1;\"\n",
    "    )\n",
    "    statement_id = statement_response['Id']\n",
    "    print(f\"Data API Statement ID: {statement_id}\")\n",
    "\n",
    "    # Wait for statement to complete (for demo purposes)\n",
    "    import time\n",
    "    status = ''\n",
    "    while status not in ['FINISHED', 'FAILED', 'ABORTED']:\n",
    "        time.sleep(1)\n",
    "        description_response = redshift_data_client.describe_statement(Id=statement_id)\n",
    "        status = description_response['Status']\n",
    "        print(f\"Statement status: {status}\")\n",
    "\n",
    "    if status == 'FINISHED':\n",
    "        result_response = redshift_data_client.get_statement_result(Id=statement_id)\n",
    "        print(\"Data API Results:\")\n",
    "        # Data API results are in a specific format, you'd usually parse them\n",
    "        if result_response and result_response.get('Records'):\n",
    "            # Extract column names\n",
    "            column_names = [col['label'] for col in result_response['ColumnMetadata']]\n",
    "            print(column_names)\n",
    "            for record in result_response['Records']:\n",
    "                # Each record is a list of dictionaries, where each dict has a key matching the data type (e.g., 'longValue', 'stringValue')\n",
    "                row_values = [list(col.values())[0] for col in record]\n",
    "                print(row_values)\n",
    "        else:\n",
    "            print(\"No results from Data API.\")\n",
    "    else:\n",
    "        print(f\"Data API query failed or was aborted: {description_response.get('Error')}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error using Redshift Data API: {e}\")\n",
    "    print(\"Ensure the IAM user/role has 'redshift-data:*' permissions.\")\n",
    "\n",
    "\n",
    "# --- Cleanup ---\n",
    "print(\"\\n--- Cleaning up ---\")\n",
    "try:\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(\"Connection closed.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during cleanup: {e}\")\n",
    "\n",
    "print(\"\\n--- Demo Complete! ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
