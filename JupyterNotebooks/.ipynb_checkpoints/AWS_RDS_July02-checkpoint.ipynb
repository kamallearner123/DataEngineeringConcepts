{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27717185-29dd-4ea8-a7cd-e3706b24948b",
   "metadata": {},
   "source": [
    "# Check installation of packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "554761a6-d32d-4cfd-acab-03a2fa0b1283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26c8720-6603-4a77-a0b6-e41dcd9978f2",
   "metadata": {},
   "source": [
    "# Create Boto3 session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9211a16f-b656-4c61-b0bf-b6914ddd3950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'kkm-unique-s3-bucket-name-12345', 'CreationDate': datetime.datetime(2025, 6, 26, 8, 13, 56, tzinfo=tzutc())}\n",
      "{'Name': 'kkm-unique-test-bucket-2025-06-26-py', 'CreationDate': datetime.datetime(2025, 6, 26, 12, 56, 57, tzinfo=tzutc())}\n",
      "{'Name': 'kkm2-unique-test-bucket-2025-06-26-py', 'CreationDate': datetime.datetime(2025, 6, 26, 12, 58, 13, tzinfo=tzutc())}\n",
      "{'Name': 'kkm2-unique-test-bucket-2025-06-26-py-1', 'CreationDate': datetime.datetime(2025, 6, 26, 15, 12, 49, tzinfo=tzutc())}\n",
      "{'Name': 'kkmjuly01', 'CreationDate': datetime.datetime(2025, 7, 1, 14, 46, 5, tzinfo=tzutc())}\n",
      "{'Name': 'kkms3bucket', 'CreationDate': datetime.datetime(2025, 6, 25, 15, 9, 35, tzinfo=tzutc())}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'S3' object has no attribute 'up'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m s3_client.list_buckets()[\u001b[33m'\u001b[39m\u001b[33mBuckets\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43ms3_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mup\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/virtualEnvs/DeepCode/lib/python3.13/site-packages/botocore/client.py:950\u001b[39m, in \u001b[36mBaseClient.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    947\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    948\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m event_response\n\u001b[32m--> \u001b[39m\u001b[32m950\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m    951\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    952\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'S3' object has no attribute 'up'"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "for i in s3_client.list_buckets()['Buckets']:\n",
    "    print(i)\n",
    "\n",
    "s3_client.up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad5ab8c-7e9b-42d6-b2e8-8d3c61c384f2",
   "metadata": {},
   "source": [
    "# Create Redshift connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d38a369-1f27-4bae-8610-3e754364330c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema               Table Name                     Owner          \n",
      "-----------------------------------------------------------------\n",
      "pg_internal          redshift_auto_health_check_87173619 rdsdb          \n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "#default-workgroup.310879042055.us-east-1.redshift-serverless.amazonaws.com:5439/dev\n",
    "rs_conn = psycopg2.connect(\n",
    "    dbname='dev',\n",
    "    host='default-workgroup.310879042055.us-east-1.redshift-serverless.amazonaws.com',\n",
    "    port='5439',\n",
    "    user='admin',\n",
    "    password='Redshift123')\n",
    "rs_conn.autocommit = True\n",
    "\n",
    "curser = rs_conn.cursor()\n",
    "sql_query = \"\"\"\n",
    "        SELECT\n",
    "            schemaname AS schema_name,\n",
    "            tablename AS table_name,\n",
    "            tableowner AS table_owner\n",
    "        FROM\n",
    "            pg_tables\n",
    "        WHERE\n",
    "            schemaname NOT IN ('pg_catalog', 'pg_toast', 'information_schema')\n",
    "        \"\"\"\n",
    "sql_query += \" ORDER BY schemaname, tablename;\"\n",
    "curser.execute(sql_query)\n",
    "tables = curser.fetchall()\n",
    "print(f\"{'Schema':<20} {'Table Name':<30} {'Owner':<15}\")\n",
    "print(\"-\" * 65)\n",
    "for table in tables:\n",
    "    print(f\"{table[0]:<20} {table[1]:<30} {table[2]:<15}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e276aa-0118-4c5b-981e-8a5a1c98d816",
   "metadata": {},
   "source": [
    "# Create a new table in Redshift: Copy from s3 to Redshift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "309d1bb6-98aa-427d-9788-8565551b033b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'demo_db_july02' already exists. Skipping creation.\n",
      "Schema 'demo_schema' is ready.\n",
      "\n",
      "Attempting to create table demo_schema.demo_table...\n",
      "Table 'demo_schema.demo_table' created or already exists.\n",
      "\n",
      "Truncating table demo_schema.demo_table for a fresh load...\n",
      "Table truncated.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'S3_REGION' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m#iam_role_arn = 'arn:aws:iam::YOUR_AWS_ACCOUNT_ID:role/YourRedshiftClusterIAMRole' # REPLACE THIS\u001b[39;00m\n\u001b[32m     67\u001b[39m iam_role_arn = \u001b[33m'\u001b[39m\u001b[33marn:aws:account::310879042055:account\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     69\u001b[39m copy_sql = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[33m        COPY \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_table_name\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33m        FROM \u001b[39m\u001b[33m'\u001b[39m\u001b[33ms3://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBUCKET_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mREMOTE_FILE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[33m        IAM_ROLE \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00miam_role_arn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[33m        CSV\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[33m        IGNOREHEADER 1\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[33m        DELIMITER \u001b[39m\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[33m        REGION \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mS3_REGION\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[33m        TIMEFORMAT \u001b[39m\u001b[33m'\u001b[39m\u001b[33mYYYY-MM-DD HH:MI:SS\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[33m        TRUNCATECOLUMNS\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[33m        EMPTYASNULL\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[33m        MAXERROR 5;\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[33m        \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mExecuting COPY command to load data into \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_table_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     83\u001b[39m \u001b[38;5;28mprint\u001b[39m(copy_sql) \u001b[38;5;66;03m# Print the SQL for debugging/review\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'S3_REGION' is not defined"
     ]
    }
   ],
   "source": [
    "BUCKET_NAME = 'kkm2-unique-test-bucket-2025-06-26-py-1'\n",
    "REMOTE_FILE_PATH = \"daniel/flights.csv\"\n",
    "db_name = \"demo_db_july02\"\n",
    "schema_name = \"demo_schema\"\n",
    "table_name = \"demo_table\"\n",
    "S3_REGION = \"us-east-1\"\n",
    "\n",
    "def get_rs_cursor():\n",
    "    rs_conn = psycopg2.connect(\n",
    "    dbname='dev',\n",
    "    host='default-workgroup.310879042055.us-east-1.redshift-serverless.amazonaws.com',\n",
    "    port='5439',\n",
    "    user='admin',\n",
    "    password='Redshift123')\n",
    "    rs_conn.autocommit = True\n",
    "    \n",
    "    cursor = rs_conn.cursor()\n",
    "    return cursor\n",
    "\n",
    "def get_s3_client():\n",
    "    return boto3.client('s3')\n",
    "\n",
    "# 1) Get cursor\n",
    "cur = get_rs_cursor()\n",
    "\n",
    "# 2) Get file from S3\n",
    "s3_client = get_s3_client()\n",
    "\n",
    "# 3) Check file from s3\n",
    "resp = s3_client.head_object(Bucket=BUCKET_NAME,\n",
    "                               Key=REMOTE_FILE_PATH)\n",
    "\n",
    "# 4) Create a DB in redshift\n",
    "# Check if database already exists\n",
    "cur.execute(f\"SELECT 1 FROM pg_database WHERE datname = '{db_name}';\")\n",
    "if cur.fetchone():\n",
    "    print(f\"Database '{db_name}' already exists. Skipping creation.\")\n",
    "else:\n",
    "    print(f\"\\nAttempting to create new database: {db_name}...\")\n",
    "    create_db_sql = f\"CREATE DATABASE {db_name};\"\n",
    "    cur.execute(create_db_sql)\n",
    "\n",
    "# 5) Create table in redshift\n",
    "cur.execute(f\"CREATE SCHEMA IF NOT EXISTS {schema_name};\")\n",
    "print(f\"Schema '{schema_name}' is ready.\")\n",
    "\n",
    "# Define table creation SQL based on sample_data.csv\n",
    "# IMPORTANT: Adjust column definitions to match your actual CSV data types\n",
    "create_table_sql = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {schema_name}.{table_name} (\n",
    "    year INT,\n",
    "    month VARCHAR(256),\n",
    "    count INT\n",
    ");\n",
    "\"\"\"\n",
    "print(f\"\\nAttempting to create table {schema_name}.{table_name}...\")\n",
    "cur.execute(create_table_sql)\n",
    "print(f\"Table '{schema_name}.{table_name}' created or already exists.\")\n",
    "\n",
    "# 6) Copying s3 file to redshift\n",
    "full_table_name = f\"{schema_name}.{table_name}\"\n",
    "print(f\"\\nTruncating table {full_table_name} for a fresh load...\")\n",
    "truncate_sql = f\"TRUNCATE {full_table_name};\"\n",
    "cur.execute(truncate_sql)\n",
    "print(\"Table truncated.\")\n",
    "\n",
    "#iam_role_arn = 'arn:aws:iam::YOUR_AWS_ACCOUNT_ID:role/YourRedshiftClusterIAMRole' # REPLACE THIS\n",
    "iam_role_arn = 'arn:aws:account::310879042055:account'\n",
    "\n",
    "copy_sql = f\"\"\"\n",
    "        COPY {full_table_name}\n",
    "        FROM 's3://{BUCKET_NAME}/{REMOTE_FILE_PATH}'\n",
    "        IAM_ROLE '{iam_role_arn}'\n",
    "        CSV\n",
    "        IGNOREHEADER 1\n",
    "        DELIMITER ','\n",
    "        REGION '{S3_REGION}'\n",
    "        TIMEFORMAT 'YYYY-MM-DD HH:MI:SS'\n",
    "        TRUNCATECOLUMNS\n",
    "        EMPTYASNULL\n",
    "        MAXERROR 5;\n",
    "        \"\"\"\n",
    "print(f\"\\nExecuting COPY command to load data into {full_table_name}...\")\n",
    "print(copy_sql) # Print the SQL for debugging/review\n",
    "cur.execute(copy_sql)\n",
    "conn.commit() # Important: COPY is part of a transaction, commit it.\n",
    "print(f\"Data successfully copied from s3://{BUCKET_NAME}/{REMOTE_FILE_PATH} to {full_table_name}.\")\n",
    "\n",
    "# Optional: Verify load status or errors\n",
    "cur.execute(\"SELECT * FROM stl_load_errors ORDER BY starttime DESC LIMIT 5;\")\n",
    "load_errors = cur.fetchall()\n",
    "if load_errors:\n",
    "    print(\"\\n--- Recent Load Errors (Last 5) ---\")\n",
    "    for error in load_errors:\n",
    "        print(error)\n",
    "else:\n",
    "    print(\"\\nNo recent load errors detected for this COPY operation.\")\n",
    "\n",
    "# Optional: Count rows after load\n",
    "cur.execute(f\"SELECT COUNT(*) FROM {full_table_name};\")\n",
    "row_count = cur.fetchone()[0]\n",
    "print(f\"Total rows in {full_table_name} after load: {row_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb3d14e-bb04-490b-9275-468240ff9719",
   "metadata": {},
   "source": [
    "# Query an entire Redshift "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aa0a74-f12b-494f-8f71-7c751abab7b2",
   "metadata": {},
   "source": [
    "# Query a table from Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be7ccee-35ab-4825-8566-ec972107d4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (DeepCode)",
   "language": "python",
   "name": "deepcode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
